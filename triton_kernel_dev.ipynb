{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# Kernel Development & Optimizations with Triton\n",
    "\n",
    "[OpenAI Triton](https://github.com/triton-lang/triton) is an open-source programming language designed to simplify GPU programming for high-performance tasks, particularly in AI applications, which has been supported by AMD GPUs. This tutorial will demonstrate how to set up the Triton development environment and optimize Triton Kernel perofrmance on AMD GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbb5a8",
   "metadata": {},
   "source": [
    "### Install OpenAI Triton\n",
    "\n",
    "#### 1. Uninstall the old Triton\n",
    "It is strongly recommended to use the latest version Triton in your project, because AMD and other vendors are updating their optimization passes and algorithms frequently in [OpenAI Triton](https://github.com/triton-lang/triton), which can help improve your Triton kernel performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "#### 2. Install OpenAI Triton from source codes\n",
    "The detailed steps to install Triton have been listed here. If meeting any questions or issues when building Triton,please submit them in [Triton Issues](https://github.com/triton-lang/triton/issues).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove existing Triton folder if it exists\n",
    "if [ -d \"triton\" ]; then\n",
    "    echo \"Removing existing triton directory...\"\n",
    "    rm -rf triton\n",
    "fi\n",
    "\n",
    "# Clone Triton repo\n",
    "git clone https://github.com/triton-lang/triton.git\n",
    "\n",
    "# Install dependencies and Triton from source (non-editable install)\n",
    "cd triton\n",
    "pip install -r python/requirements.txt\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b7a4",
   "metadata": {},
   "source": [
    "### Validate Triton on AMD GPU\n",
    "Once Triton is installed on the machine successfully, we can validate whether it can work well or not on AMD GPU machine. By running the below vector-add sample through python, we can find that Triton kernel can give the sample result with Torch APIs, which means Triton can work well on AMD GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # *Pointer* to first input vector.\n",
    "               y_ptr,  # *Pointer* to second input vector.\n",
    "               output_ptr,  # *Pointer* to output vector.\n",
    "               n_elements,  # Size of the vector.\n",
    "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "               # NOTE: `constexpr` so it can be used as a shape value.\n",
    "               ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Let's also declare a helper function to (1) allocate the `z` tensor\n",
    "# and (2) enqueue the above kernel with appropriate grid/block sizes:\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output\n",
    "\n",
    "\n",
    "# %%\n",
    "# We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n",
    "\n",
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device=DEVICE)\n",
    "y = torch.rand(size, device=DEVICE)\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c529af",
   "metadata": {},
   "source": [
    "The output log is:\n",
    "    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n",
    "    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n",
    "    The maximum difference between torch and triton is 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "## Optimize Triton codes on AMD GPUs\n",
    "\n",
    "The softmax function, often used in classification CNN models and even Transformer based LLM models,converts raw output scores or logits, into probabilities by taking the exponential of each value and normalizing these values by dividing by the sum of all the exponentials. This process ensures that the output values are in the range (0,1) and sum up to 1, making them interpretable as probabilities. PyTorch has implemented it as [a standard API](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). \n",
    "\n",
    "### Naive version \n",
    "\n",
    "According to the softmax algorithm definition, we implemented the naive version Triton kernel. To get the maximum data and the corresponding sum of all the exponentials, 2 for-loops are implemented in this version kernel, and there is still 1 for-loop to calculate the final softmax result. So total 3 loops are used in this kernel. \n",
    "\n",
    "Assuming that we need to test this kernel performance on an 8192x8192 tensor, the block size of col dimension is 256, after running warmup to avoid the kernel compilation time is included in the final performance data, we can get the navive version performance data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    in_max = -float('inf')\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_max = tl.maximum(in_max, tl.max(in_data, axis=-1))\n",
    "    \n",
    "    in_exp_sum = 0.0\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_exp_sum = in_exp_sum + tl.sum(tl.exp(in_data - in_max), axis=-1)\n",
    "    \n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n",
    "        in_exp = tl.exp(in_data - in_max)\n",
    "        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "output_triton = torch.empty_like(x)\n",
    "BLOCK_SIZE = 256\n",
    "temp = torch.randn(n_rows, n_cols, device=DEVICE)\n",
    "softmax_kernel_naive[(n_rows,)](\n",
    "        temp,\n",
    "        output_triton,\n",
    "        temp.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")#warmup\n",
    "torch.cuda.empty_cache() #clean cache\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "softmax_kernel_naive[(n_rows,)](\n",
    "        x,\n",
    "        output_triton,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f'Softmax Triton Naive Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208055b",
   "metadata": {},
   "source": [
    "\n",
    "### Online-softmax version \n",
    "Triton language is easy to implement a algorothm for GPU developpers. In order to have the better perofrmance of current kernel, we can first figure out whether there is a more efficient algorithm/solution. If so, we had better try the new algorithm in our Triton Kernel. To reduce the memory access caused by 3 for-loops in naive SoftMax algorithm, a new algorithm of on-line softmax has been proposed in [this paper](https://arxiv.org/pdf/1805.02867).\n",
    "\n",
    "accoding to online-softmax algorithm, we made some small modifications on navie version kernel, as shown in below codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d608d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel_v1(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    in_max = -float('inf')\n",
    "    in_exp_sum = 0.0\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_max_new = tl.maximum(in_max, tl.max(in_data, axis=-1))\n",
    "        in_exp_sum = in_exp_sum * tl.exp(in_max - in_max_new) + tl.sum(tl.exp(in_data - in_max_new), axis=-1)\n",
    "        in_max = in_max_new\n",
    "    \n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n",
    "        in_exp = tl.exp(in_data - in_max)\n",
    "        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "output = torch.empty_like(x)\n",
    "BLOCK_SIZE = 256\n",
    "temp = torch.randn(n_rows, n_cols, device=DEVICE)\n",
    "softmax_kernel_v1[(n_rows,)](\n",
    "        temp,\n",
    "        output,\n",
    "        temp.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")#warmup\n",
    "torch.cuda.empty_cache() #clean cache\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "softmax_kernel_v1[(n_rows,)](\n",
    "        x,\n",
    "        output,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "print(f'Softmax Triton V1 Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df425375",
   "metadata": {},
   "source": [
    "\n",
    "### Fused-softmax version\n",
    "OpenAI Triton provided a reference softmax sample codes with the name of \"fused-softmax\". Based on online softmax, it continued to simplify the maxmumim data calculation, which can remove 1 for-loop. it also ask the compiler to use more threads per row by increasing the number of warps, which is often tuned for better performance. and finally it improved the kernel lauching scheme by the GPU hardware properties, which can have the higher GPU kernel occupancy and better performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908')\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "# Allocate output\n",
    "y = torch.empty_like(x)\n",
    "\n",
    "# The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "# Another trick we can use is to ask the compiler to use more threads per row by\n",
    "# increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "num_warps = 8\n",
    "\n",
    "# Number of software pipelining stages.\n",
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "# pre-compile kernel to get register usage and compute thread occupancy.\n",
    "kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "\n",
    "if is_hip():\n",
    "    # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "    # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "    # ISA SECTION (3.6.4 for CDNA3)\n",
    "    # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "    # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "    # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "    # not required to be equal numbers of both types.\n",
    "    if is_cdna():\n",
    "        NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "    # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "    # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "    # execute on a CU (multi-processor)  in parallel.\n",
    "    MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "    max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "    occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "else:\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    \n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "\n",
    "num_programs = min(num_programs, n_rows)\n",
    "\n",
    "# Create a number of persistent programs.\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f'Softmax Triton V2 Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "\n",
    "## Summary \n",
    "Through this tutorial, developer has already know how to develop and optimize Triton kernel on AMD GPUs. If developer would like to study more about OpenAI Triton itself, OpenAI Triton [official document](https://triton-lang.org/main/index.html) can be very useful. You can find more information about AMD Triton from [this document](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel.html) and [this blog](https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html).We hope that this tutorial will encourage you to tune, test, and contribute to Triton on AMD GPUs, and help us shape the future of AI acceleration.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ada006",
   "metadata": {},
   "source": [
    "Happy coding! If you encounter issues or have questions, donâ€™t hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comfyui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
