{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LLMs in ANY Environment with OpenEnv\n",
    "\n",
    "## üéØ The Vision\n",
    "\n",
    "Imagine training language models in:\n",
    "- üé∞ **Card games** (BlackJack, Poker, Uno)\n",
    "- ‚ôüÔ∏è **Board games** (Chess, Go, Connect Four)\n",
    "- üìà **Trading simulations** (realistic market environments)\n",
    "- üéÆ **Atari games** (Pong, Breakout, Space Invaders)\n",
    "- üíª **Code execution environments** (interactive debugging)\n",
    "- ü§ñ **Robotics simulations** (MuJoCo, PyBullet)\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Every RL environment has different APIs:\n",
    "- ‚ùå OpenSpiel uses C++ bindings\n",
    "- ‚ùå Atari needs ALE (Arcade Learning Environment)\n",
    "- ‚ùå Trading sims have custom interfaces\n",
    "- ‚ùå Each requires different dependencies, versions, OS compatibility\n",
    "- ‚ùå No isolation ‚Üí crashes can corrupt your system\n",
    "\n",
    "**You spend more time wrestling with environments than training models.**\n",
    "\n",
    "---\n",
    "\n",
    "### The Solution: OpenEnv - A Universal Spec\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 10px; color: white; margin: 20px 0;'>\n",
    "    <h3 style='margin-top: 0;'>üöÄ OpenEnv = Universal RL Environment Interface</h3>\n",
    "    <p style='font-size: 18px; line-height: 1.8;'>\n",
    "        <b>OpenEnv is not a game engine.</b><br>\n",
    "        It's a <b>specification</b> that wraps ANY RL environment with a clean, unified API.\n",
    "    </p>\n",
    "    <ul style='font-size: 16px; line-height: 1.8;'>\n",
    "        <li><b>70+ environments</b> (OpenSpiel, Atari, FinRL, and more)</li>\n",
    "        <li><b>Unified Simplified API:</b> <code>reset()</code>, <code>step(action)</code>, <code>state()</code></li>\n",
    "        <li><b>HTTP-based</b> ‚Üí language-agnostic (Python, Rust, JavaScript, anything)</li>\n",
    "        <li><b>Docker-isolated</b> ‚Üí reproducible, secure, no dependency hell</li>\n",
    "    </ul>\n",
    "    <p style='font-size: 16px; margin-top: 15px;'>\n",
    "        <b>One interface. Any environment. Zero setup.</b>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "In this tutorial, you'll:\n",
    "1. üîå **Explore OpenEnv** - Connect to BlackJack, see how the spec works\n",
    "2. üé≤ **Benchmark policies** - Test random vs heuristic strategies\n",
    "3. üß† **Learn about GRPO** - Brief intro to the training algorithm\n",
    "4. ‚ö° **Train with Forge** - Use PyTorch's agentic RL library\n",
    "5. üìä **Compare results** - Measure improvement\n",
    "6. üîÑ **Switch environments** - Show how to train on different games\n",
    "\n",
    "**This uses production code.** Same implementation as `apps/grpo/blackjack_main_fixed.py`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Resources\n",
    "- üì¶ [OpenEnv GitHub](https://github.com/meta-pytorch/OpenEnv) - Universal RL environment spec\n",
    "- üìÑ [GRPO Paper (arXiv:2402.03300)](https://arxiv.org/abs/2402.03300) - Group Relative Policy Optimization\n",
    "- üîß [Forge GitHub](https://github.com/meta-pytorch/torchforge) - PyTorch-native agentic RL library\n",
    "- üìñ [Forge Docs](https://meta-pytorch.org/torchforge/) - Full documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Part 1: Exploring OpenEnv\n",
    "\n",
    "Let's connect to a BlackJack environment and explore the OpenEnv spec.\n",
    "\n",
    "### Start the Server\n",
    "\n",
    "<div style='background: #fff3cd; padding: 15px; border-radius: 8px; border-left: 5px solid #ffc107; margin: 20px 0;'>\n",
    "    <b>‚ö†Ô∏è Note:</b> Start the OpenEnv server in a separate terminal:\n",
    "    <pre style='margin-top: 10px; background: white; padding: 10px; border-radius: 5px;'>\n",
    "# Set your OpenEnv path\n",
    "export OPENENV_PATH=\"/path/to/OpenEnv/src\"\n",
    "export PYTHONPATH=\"${OPENENV_PATH}:${PYTHONPATH}\"\n",
    "\n",
    "# Start BlackJack server\n",
    "OPENSPIEL_GAME=blackjack python -m envs.openspiel_env.server.app --port 8004</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for Jupyter\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Fix for Monarch/Torchstore Rust bindings in Jupyter\n",
    "conda_prefix = os.environ.get('CONDA_PREFIX', sys.prefix)\n",
    "lib_path = f\"{conda_prefix}/lib\"\n",
    "\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    if lib_path not in os.environ['LD_LIBRARY_PATH']:\n",
    "        os.environ['LD_LIBRARY_PATH'] = f\"{lib_path}:{os.environ['LD_LIBRARY_PATH']}\"\n",
    "else:\n",
    "    os.environ['LD_LIBRARY_PATH'] = lib_path\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to OpenEnv\n",
    "\n",
    "Let's connect to the BlackJack environment and explore its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sysimport osfrom pathlib import Path# Add OpenEnv to path (update this to your OpenEnv installation)openenv_path = os.environ.get('OPENENV_PATH', '/path/to/OpenEnv/src')if openenv_path not in sys.path:    sys.path.insert(0, openenv_path)from envs.openspiel_env import OpenSpielEnv, OpenSpielActionfrom grpo_utils import show_openenv_observation# Connect to environmentenv = OpenSpielEnv(base_url=\"http://localhost:8004\")print(\"üé∞ Connected to BlackJack environment\")print(\"\\nüìç Resetting environment...\\n\")# Reset and observeresult = env.reset()show_openenv_observation(result.observation)env.close()print(\"\\n‚úÖ OpenEnv interface exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "You just saw the **OpenEnv spec** in action:\n",
    "\n",
    "```python\n",
    "# Universal interface - works for ANY environment\n",
    "result = env.reset()              # Start episode\n",
    "result = env.step(action)         # Take action\n",
    "state = env.state()               # Get environment state\n",
    "env.close()                       # Cleanup\n",
    "```\n",
    "\n",
    "**Key observations:**\n",
    "- `legal_actions`: What actions the agent can take\n",
    "- `info_state`: Numeric observation vector\n",
    "- `game_phase`: Current phase of the game\n",
    "- `reward`: Outcome (+1 win, -1 loss, 0 push)\n",
    "\n",
    "This same interface works for **70+ different environments**. Change the server, everything else stays the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Part 2: Benchmarking Baseline Policies\n",
    "\n",
    "Before training an LLM, let's see how simple policies perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grpo_utils import play_random_policyprint(\"üé≤ Running random policy baseline...\\n\")# Play 100 games with random actionsstats = play_random_policy(\"http://localhost:8004\", num_games=100)print(\"\\nüìä Random Policy Results:\")print(f\"   Games played: {stats['total_games']}\")print(f\"   Wins: {stats['wins']}\")print(f\"   Losses: {stats['losses']}\")print(f\"   Pushes: {stats['pushes']}\")print(f\"   Win rate: {stats['win_rate']:.1%}\")print(\"\\nüìù Note: Optimal BlackJack strategy achieves ~43% win rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge\n",
    "\n",
    "Random policy performs poorly (~30-35% win rate).\n",
    "\n",
    "**Can we train an LLM to do better?**\n",
    "\n",
    "That's where **GRPO** comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Part 3: Understanding Reinforcement Learning & GRPO\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #e66465 0%, #9198e5 100%); padding: 25px; border-radius: 10px; color: white; margin: 20px 0; border: 3px solid #fff;'>\n",
    "    <h3 style='margin-top: 0;'>üìö Section Inspired by Unsloth</h3>\n",
    "    <p style='font-size: 16px; line-height: 1.8;'>\n",
    "        This section is heavily inspired by the excellent <a href='https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide' style='color: #fff; text-decoration: underline;'><b>Unsloth RL Guide</b></a>.\n",
    "        <br><br>\n",
    "        Unsloth has done an amazing job making RL accessible and intuitive. We highly recommend reading their full guide for deeper insights and practical tips!\n",
    "        <br><br>\n",
    "        üôè <b>Big thanks to the Unsloth team</b> for their educational approach to RL.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What is Reinforcement Learning?\n",
    "\n",
    "<div style='background: #f8f9fa; padding: 20px; border-radius: 10px; border-left: 5px solid #6c757d; margin: 20px 0;'>\n",
    "    <h4 style='margin-top: 0;'>The Core Idea (It's Simpler Than You Think!)</h4>\n",
    "    <p style='font-size: 16px; line-height: 1.8;'>\n",
    "        The goal of RL is extremely simple:\n",
    "    </p>\n",
    "    <ul style='font-size: 16px; line-height: 1.8;'>\n",
    "        <li>‚úÖ <b>Increase the chance of seeing \"good\" outcomes</b></li>\n",
    "        <li>‚ùå <b>Decrease the chance of seeing \"bad\" outcomes</b></li>\n",
    "    </ul>\n",
    "    <p style='font-size: 16px; margin-top: 10px;'>\n",
    "        That's it! Everything else is just details about what \"good\" and \"bad\" mean, and how to increase/decrease their probabilities.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "#### A Simple Example: Learning \"2 + 2 = ?\"\n",
    "\n",
    "Imagine an untrained language model trying to answer \"What is 2+2?\". It might output:\n",
    "\n",
    "```\n",
    "0, cat, -10, 1928, 3, A, B, 122, 17, 182, 172, A, C, BAHS, %$, #, 9, -192, 12.31, ...\n",
    "```\n",
    "\n",
    "Then suddenly: **4** ‚úì\n",
    "\n",
    "The reward signals would be:\n",
    "```\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... then 1\n",
    "```\n",
    "\n",
    "**This is the key insight:** By patience (or \"luck\"), if the correct answer has *any* non-zero probability, RL will eventually find it. The trick is:\n",
    "1. While waiting, we learn from **bad answers** ‚Üí tell model \"don't do this\"\n",
    "2. When we find **good answers** ‚Üí tell model \"do more of this\"\n",
    "\n",
    "This is why I like to call it **\"Patience Is All You Need\"** for RL.\n",
    "\n",
    "---\n",
    "\n",
    "### From PPO to GRPO: The Evolution\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 10px; color: white; margin: 20px 0;'>\n",
    "    <h4 style='margin-top: 0;'>üìú The Algorithm Evolution</h4>\n",
    "    \n",
    "<table style='width: 100%; color: white; margin-top: 15px;'>\n",
    "<tr>\n",
    "    <td style='padding: 8px; border-bottom: 1px solid rgba(255,255,255,0.3);'><b>RLHF + PPO</b> (OpenAI ChatGPT)</td>\n",
    "    <td style='padding: 8px; border-bottom: 1px solid rgba(255,255,255,0.3);'>Needed 3 models: Policy, Reference, Value Model</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td style='padding: 8px;'><b>GRPO</b> (DeepSeek R1)</td>\n",
    "    <td style='padding: 8px;'>Only needs 2 models: Policy + Reference<br>‚Üí <b>Much more efficient!</b></td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "**What GRPO removes:**\n",
    "- ‚ùå **Value Model** ‚Üí Replaced with group statistics\n",
    "- ‚ùå **Reward Model** ‚Üí Replaced with simple reward functions\n",
    "\n",
    "**Why this matters:**\n",
    "- üíæ Less memory usage\n",
    "- ‚ö° Faster training\n",
    "- üéØ Easier to implement\n",
    "\n",
    "---\n",
    "\n",
    "### GRPO: Group Relative Policy Optimization\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 25px; border-radius: 10px; color: white; margin: 20px 0;'>\n",
    "    <h4 style='margin-top: 0;'>Why \"Group Relative\"?</h4>\n",
    "    <p style='font-size: 16px; line-height: 1.8;'>\n",
    "        Instead of training a separate Value Model to estimate \"how good is this state?\", \n",
    "        GRPO uses a clever trick: <b>sample the model multiple times</b> and compare answers within the group.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "**Example: Training on \"What is 2+2?\"**\n",
    "\n",
    "1. **Generate multiple responses** (e.g., 4 samples):\n",
    "   - Response 1: \"4\" ‚Üí reward = +1 (correct!)\n",
    "   - Response 2: \"3\" ‚Üí reward = 0 (close, but wrong)\n",
    "   - Response 3: \"D\" ‚Üí reward = -1 (nonsense)\n",
    "   - Response 4: \"C\" ‚Üí reward = -1 (nonsense)\n",
    "\n",
    "2. **Calculate group statistics:**\n",
    "   - Mean reward: (-1 + -1 + 0 + 1) / 4 = -0.25\n",
    "   - Standard deviation: ~0.83\n",
    "\n",
    "3. **Compute advantages** (Z-score normalization):\n",
    "   - Response 1: +1.5 (much better than average!)\n",
    "   - Response 2: +0.3 (slightly better)\n",
    "   - Response 3: -0.9 (worse than average)\n",
    "   - Response 4: -0.9 (worse than average)\n",
    "\n",
    "4. **Update model:**\n",
    "   - Increase probability of generating \"4\"\n",
    "   - Slightly increase \"3\" (it's closer than nonsense)\n",
    "   - Decrease probability of generating \"D\" and \"C\"\n",
    "\n",
    "This is **group-relative** because we're comparing within the group, not to an absolute baseline!\n",
    "\n",
    "---\n",
    "\n",
    "### Reward Functions: The Secret Sauce\n",
    "\n",
    "Reward functions tell the model what's \"good\" and what's \"bad\". They can be simple or complex:\n",
    "\n",
    "**For BlackJack (what we're using):**\n",
    "```python\n",
    "def evaluate_response(prompt, response, game_reward):\n",
    "    reward = float(game_reward)  # +1 (win), -1 (loss), 0 (push)\n",
    "    \n",
    "    # Reward shaping: Scale up wins\n",
    "    if game_reward > 0:\n",
    "        reward = 2.0  # Wins are more valuable\n",
    "    elif game_reward == 0:\n",
    "        reward = 0.5  # Pushes better than losses\n",
    "    \n",
    "    return reward\n",
    "```\n",
    "\n",
    "**For Math Problems:**\n",
    "- If answer is a number: +1\n",
    "- If answer matches ground truth: +3\n",
    "- If no number detected: -1\n",
    "- **Total reward:** Sum of all criteria\n",
    "\n",
    "**For Email Automation:**\n",
    "- Contains required keyword: +1\n",
    "- Matches ideal response: +1\n",
    "- Too long: -1\n",
    "- Includes recipient name: +1\n",
    "- Has signature block: +1\n",
    "\n",
    "The key is: **Reward functions must be verifiable**. You can't subjectively judge \"is this creative?\" but you can verify \"is this answer correct?\"\n",
    "\n",
    "---\n",
    "\n",
    "### The Training Process (Simplified)\n",
    "\n",
    "```\n",
    "1. Play game ‚Üí Get action \"HIT\" or \"STAND\"\n",
    "   ‚Üì\n",
    "2. Game ends ‚Üí Observe reward (+1 win, -1 loss, 0 push)\n",
    "   ‚Üì\n",
    "3. Repeat 4-8 times for the same question (group)\n",
    "   ‚Üì\n",
    "4. Calculate group statistics (mean, std)\n",
    "   ‚Üì\n",
    "5. Compute advantages (which answers were better/worse than average?)\n",
    "   ‚Üì\n",
    "6. Update model: increase good action probability, decrease bad\n",
    "   ‚Üì\n",
    "7. Repeat thousands of times ‚Üí Model learns strategy!\n",
    "```\n",
    "\n",
    "**Key insight:** Over time, the model learns not just \"what to do\" but also *why* (the reasoning process). This is how DeepSeek R1 developed its famous `<think>` tokens!\n",
    "\n",
    "---\n",
    "\n",
    "### Forge: PyTorch-Native Agentic RL Infrastructure\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #20c997 0%, #17a2b8 100%); padding: 20px; border-radius: 10px; color: white; margin: 20px 0;'>\n",
    "    <h4 style='margin-top: 0;'>What is Forge?</h4>\n",
    "    <p style='font-size: 16px; line-height: 1.6;'>\n",
    "        <b>Forge</b> is PyTorch's official library for training agentic RL models. It handles all the distributed systems complexity so you can focus on algorithms.\n",
    "    </p>\n",
    "    <ul style='font-size: 15px; line-height: 1.7;'>\n",
    "        <li><b>Generator (vLLM):</b> Fast LLM inference with automatic batching</li>\n",
    "        <li><b>RLTrainer:</b> Distributed training with FSDP across GPUs</li>\n",
    "        <li><b>ReplayBuffer:</b> Stores episodes for off-policy learning</li>\n",
    "        <li><b>ReferenceModel:</b> Keeps original model for KL penalty</li>\n",
    "        <li><b>Torchstore:</b> Distributed weight management across replicas</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Resources:**\n",
    "- üîß [GitHub](https://github.com/meta-pytorch/torchforge) - Source code\n",
    "- üìñ [Documentation](https://meta-pytorch.org/torchforge/) - Full docs\n",
    "- üìÑ [GRPO Paper](https://arxiv.org/abs/2402.03300) - Original research\n",
    "\n",
    "**In this tutorial:** We abstract all of Forge's complexity. You just call:\n",
    "```python\n",
    "trainer = await setup_forge_training(\"config.yaml\")\n",
    "await trainer.run(steps=100)\n",
    "```\n",
    "\n",
    "Everything else happens automatically! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 4: Training with GRPO\n",
    "\n",
    "Now let's train a Qwen 1.5B model to play BlackJack using production GRPO code.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
    "‚îÉ              YOUR TRAINING LOOP                    ‚îÉ\n",
    "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
    "‚îÉ                                                    ‚îÉ\n",
    "‚îÉ  Rollouts Loop          Training Loop             ‚îÉ\n",
    "‚îÉ  ‚Ä¢ Play games           ‚Ä¢ Sample batch            ‚îÉ\n",
    "‚îÉ  ‚Ä¢ Collect episodes     ‚Ä¢ Compute loss            ‚îÉ\n",
    "‚îÉ  ‚Ä¢ Compute advantages   ‚Ä¢ Update weights          ‚îÉ\n",
    "‚îÉ  ‚Ä¢ Add to buffer        ‚Ä¢ Push to replicas        ‚îÉ\n",
    "‚îÉ                                                    ‚îÉ\n",
    "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îØ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îØ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
    "           ‚îÇ                         ‚îÇ\n",
    "      HTTP ‚îÇ                         ‚îÇ RPC\n",
    "           ‚îÇ                         ‚îÇ\n",
    "           ‚Üì                         ‚Üì\n",
    "   ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì          ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
    "   ‚îÉ   OpenEnv   ‚îÉ          ‚îÉ    Forge     ‚îÉ\n",
    "   ‚îÉ   Server    ‚îÉ          ‚îÉ   Services   ‚îÉ\n",
    "   ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ          ‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
    "```\n",
    "\n",
    "**Two concurrent loops:**\n",
    "1. **Rollouts:** Play games via OpenEnv ‚Üí collect episodes\n",
    "2. **Training:** Sample from buffer ‚Üí update policy with GRPO\n",
    "\n",
    "They run in parallel for maximum efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grpo_utils import setup_forge_trainingprint(\"üèóÔ∏è Initializing Forge infrastructure...\\n\")print(\"This will:\")print(\"  ‚Ä¢ Load the Qwen 1.5B model\")print(\"  ‚Ä¢ Initialize vLLM inference servers\")print(\"  ‚Ä¢ Setup distributed training (TorchTitan)\")print(\"  ‚Ä¢ Create replay buffer and reference model\")print(\"\\n‚è≥ This may take 1-2 minutes...\\n\")# Initialize everything with one function calltrainer = await setup_forge_training(\"blackjack.yaml\")print(\"\\n‚úÖ Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "Now we train for 100 steps. This is a shortened demo - production training uses 1000+ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting GRPO training!\\n\")\n",
    "print(\"Watch the logs to see:\")\n",
    "print(\"  ‚Ä¢ Games being played (with actions and outcomes)\")\n",
    "print(\"  ‚Ä¢ Win rate improving over time\")\n",
    "print(\"  ‚Ä¢ Training steps updating the policy\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run training (this is the production training loop!)\n",
    "results = await trainer.run(steps=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéâ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Forge services\n",
    "await trainer.shutdown()\n",
    "print(\"‚úÖ Shutdown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Part 5: The Power of OpenEnv - Switching Environments\n",
    "\n",
    "Here's the magic: **The same code works for ANY OpenEnv environment.**\n",
    "\n",
    "### Switch to Tic-Tac-Toe\n",
    "\n",
    "Just change the server:\n",
    "\n",
    "```bash\n",
    "# Terminal:\n",
    "OPENSPIEL_GAME=tic_tac_toe python -m envs.openspiel_env.server.app --port 8005\n",
    "```\n",
    "\n",
    "Update config:\n",
    "```python\n",
    "cfg.blackjack_env.server_url = \"http://localhost:8005\"\n",
    "```\n",
    "\n",
    "**Everything else stays identical.** Same GRPO code, same Forge infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "### Switch to Chess\n",
    "\n",
    "```bash\n",
    "OPENSPIEL_GAME=chess python -m envs.openspiel_env.server.app --port 8006\n",
    "```\n",
    "\n",
    "Update model and config for longer sequences, done!\n",
    "\n",
    "---\n",
    "\n",
    "### Switch to Atari\n",
    "\n",
    "```bash\n",
    "# Different OpenEnv backend\n",
    "python -m envs.atari_env.server.app --game pong --port 8007\n",
    "```\n",
    "\n",
    "Modify prompt formatting for vision inputs, same training loop!\n",
    "\n",
    "---\n",
    "\n",
    "<div style='background: #d1ecf1; padding: 20px; border-radius: 10px; border-left: 5px solid #0c5460; margin: 20px 0;'>\n",
    "    <h3 style='color: #0c5460; margin-top: 0;'>üí° The Key Insight</h3>\n",
    "    <p style='color: #0c5460; font-size: 16px;'>\n",
    "        <b>OpenEnv is a spec, not a game engine.</b><br><br>\n",
    "        Once you have a training loop that talks to OpenEnv, you can train on ANY environment that implements the spec.\n",
    "        <br><br>\n",
    "        Change one environment variable ‚Üí train on 70+ different environments.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "### 1. Scale Up Training\n",
    "\n",
    "Edit `apps/grpo/blackjack.yaml`:\n",
    "\n",
    "```yaml\n",
    "trainer:\n",
    "  training:\n",
    "    steps: 1000          # More training steps\n",
    "\n",
    "group_size: 8            # More games per rollout\n",
    "rollout_threads: 4       # Parallel rollout collection\n",
    "```\n",
    "\n",
    "Run from command line for serious training:\n",
    "\n",
    "```bash\n",
    "python -m apps.grpo.blackjack_main_fixed --config apps/grpo/blackjack.yaml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Explore Other Environments\n",
    "\n",
    "Try different OpenSpiel games:\n",
    "- `OPENSPIEL_GAME=tic_tac_toe`\n",
    "- `OPENSPIEL_GAME=connect_four`\n",
    "- `OPENSPIEL_GAME=go`\n",
    "\n",
    "Explore other OpenEnv backends:\n",
    "- Atari environments\n",
    "- FinRL trading simulations\n",
    "- Custom environments\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Customize the Training\n",
    "\n",
    "All the code is in `apps/grpo/grpo_utils.py`:\n",
    "- Modify reward shaping in `BlackJackReward.evaluate_response()`\n",
    "- Adjust advantage computation in `ComputeAdvantages.compute()`\n",
    "- Tweak GRPO loss hyperparameters (beta, KL penalty)\n",
    "- Change prompt formatting in `format_prompt()`\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "### OpenEnv\n",
    "- üì¶ [GitHub](https://github.com/meta-pytorch/OpenEnv) - Source code and examples\n",
    "- üìñ [Spec Documentation](https://github.com/meta-pytorch/OpenEnv#spec) - Full API reference\n",
    "\n",
    "### GRPO\n",
    "- üìÑ [Paper (arXiv:2402.03300)](https://arxiv.org/abs/2402.03300) - Original publication\n",
    "- üî¨ [Blog Post](https://ai.meta.com/blog/grpo/) - High-level explanation\n",
    "\n",
    "### Forge\n",
    "- üîß [GitHub](https://github.com/meta-pytorch/torchforge) - PyTorch-native agentic RL\n",
    "- üìñ [Docs](https://meta-pytorch.org/torchforge/) - Full documentation\n",
    "- üí¨ [Discussions](https://github.com/meta-pytorch/torchforge/discussions) - Community support\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "<div style='background: #d4edda; padding: 25px; border-radius: 10px; border-left: 5px solid #28a745; margin: 20px 0;'>\n",
    "    <h3 style='color: #155724; margin-top: 0;'>What You Learned</h3>\n",
    "    <ol style='color: #155724; font-size: 16px; line-height: 1.8;'>\n",
    "        <li><b>OpenEnv is a universal spec</b> for RL environments - not just games, ANY interactive environment.</li>\n",
    "        <li><b>One training loop works everywhere</b> - switch environments by changing a URL.</li>\n",
    "        <li><b>Forge abstracts distributed RL complexity</b> - focus on algorithms, not infrastructure.</li>\n",
    "        <li><b>GRPO enables stable LLM training</b> - group-relative advantages + KL penalties work.</li>\n",
    "        <li><b>Production code is accessible</b> - this notebook uses the same code as large-scale training.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin: 30px 0; text-align: center;'>\n",
    "    <h2 style='margin-top: 0;'>üéâ Congratulations!</h2>\n",
    "    <p style='font-size: 18px; line-height: 1.8;'>\n",
    "        You just trained an LLM using production GRPO code.<br>\n",
    "        You explored OpenEnv as a universal RL interface.<br>\n",
    "        You saw how Forge abstracts distributed training complexity.\n",
    "    </p>\n",
    "    <p style='font-size: 20px; margin-top: 20px;'>\n",
    "        <b>Now go train agents in ANY environment! üöÄ</b>\n",
    "    </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
