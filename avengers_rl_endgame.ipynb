{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶∏‚ôÄÔ∏è Avengers RL ‚Äî Small Specialists, United They Stand\n",
    "\n",
    "**The Final Showdown: Can specialized LoRA-trained models outperform a 20B parameter giant?**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we pit the Avengers team of specialized LoRA adapters against Thanos - a massive 20B parameter model to determine:\n",
    "\n",
    "**Can specialization + coordination out-perform brute size?**\n",
    "\n",
    "### The Contestants\n",
    "- ü¶∏ **Avengers Team**: 4 LoRA adapters, each specialized for one environment\n",
    "  - Captain America (CartPole)\n",
    "  - Iron Man (MountainCarContinuous) \n",
    "  - Thor (LunarLanderContinuous)\n",
    "  - Black Widow (BipedalWalker-v3)\n",
    "- üü£ **Thanos**: `unsloth/gpt-oss-20b` - a 20B parameter model with no task-specific training\n",
    "\n",
    "### The Challenge\n",
    "- Each contestant must generate Python strategies for each environment\n",
    "- Strategies are evaluated across multiple runs (up to 10K steps per episode)\n",
    "- Success metrics: reward, completion rate, efficiency\n",
    "\n",
    "Let's see if the lesson from comic books holds: **United we stand > Alone you fall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup & Imports\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_552/2792008468.py:27: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 10-29 22:11:27 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 22:11:30] INFO cartpole_environment.py:68: Creating CartPole-v1 environment with render_mode=None, max_steps=100000, seed=None\n",
      "[2025-10-29 22:11:30] INFO cartpole_environment.py:87: CartpoleEnvironment initialized successfully\n",
      "[2025-10-29 22:11:30] INFO sequential_environment.py:78: Creating sub-environments with seed=None\n",
      "[2025-10-29 22:11:30] INFO cartpole_environment.py:68: Creating CartPole-v1 environment with render_mode=None, max_steps=10000, seed=None\n",
      "[2025-10-29 22:11:30] INFO cartpole_environment.py:87: CartpoleEnvironment initialized successfully\n",
      "[2025-10-29 22:11:30] INFO mountaincarcontinuous_environment.py:68: Creating MountainCarContinuous-v0 environment with render_mode=None, max_steps=1000, seed=None\n",
      "[2025-10-29 22:11:30] INFO mountaincarcontinuous_environment.py:87: MountainCarContinuousEnvironment initialized successfully\n",
      "[2025-10-29 22:11:30] INFO lunarlander_environment.py:67: Creating LunarLanderContinuous-v3 environment with render_mode=None, max_steps=1000, seed=None\n",
      "[2025-10-29 22:11:30] INFO lunarlander_environment.py:82: LunarLanderEnvironment initialized successfully\n",
      "[2025-10-29 22:11:30] INFO gymnasium_environment.py:55: Creating Gymnasium environment 'BipedalWalker-v3' (render_mode=None, max_steps=1000, seed=None)\n",
      "[2025-10-29 22:11:30] INFO gymnasium_environment.py:81: GymnasiumEnvironment for 'BipedalWalker-v3' initialized\n",
      "[2025-10-29 22:11:30] INFO sequential_environment.py:102: SequentialEnvironment initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ All imports successful!\n",
      "üîß PyTorch version: 2.9.0a0+git1c57644\n",
      "üñ•Ô∏è CUDA available: True\n",
      "üñ•Ô∏è GPU: \n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch peft unsloth accelerate openenv matplotlib seaborn plotly\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from textwrap import dedent\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML/AI libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from peft import PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import create_locked_down_function\n",
    "from unsloth import check_python_modules\n",
    "\n",
    "# OpenEnv for environments\n",
    "from envs.sequential_environment import SequentialEnvironment, SequentialAction\n",
    "from envs.sequential_environment.client import SequentialEnv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üéØ All imports successful!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ∞Ô∏è Connected to OpenEnv Sequential Environment Server containing CartPole, BiPedalWalker, MountainCar, LunarLanderContinuous\n",
      "StepResult(observation=SequentialObservation(done=False, reward=0.0, metadata={}, state=[0.0, 0.0, 1.0, 0.0, -0.00039377211942337453, 1.4073383808135986, -0.03989001363515854, -0.15918950736522675, 0.00046295812353491783, 0.009035666473209858, 0.0, 0.0], phase='lunarlander', sub_observation=[-0.00039377211942337453, 1.4073383808135986, -0.03989001363515854, -0.15918950736522675, 0.00046295812353491783, 0.009035666473209858, 0.0, 0.0], episode_length=0, total_reward=0.0), reward=0.0, done=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize OpenEnv Sequential Environment Client\n",
    "max_steps = 10000\n",
    "seed = 42\n",
    "max_seq_length = 2048\n",
    "\n",
    "\n",
    "client = SequentialEnv(\n",
    "    base_url=\"http://localhost:8060\"\n",
    ")\n",
    "# If Connection fails, Please remember to start the OpenEnv Sequential Environment for Endgame by running bash OpenEnv/scripts/start_sequential_environment.sh \n",
    "# It is running on a tmux window for examination purpose, if it goes down please bring it back up.\n",
    "# Running server subprocess in Jupyter has many issues, hence running it in a tmux shell\n",
    "\n",
    "print(\"üõ∞Ô∏è Connected to OpenEnv Sequential Environment Server containing CartPole, BiPedalWalker, MountainCar, LunarLanderContinuous\")\n",
    "print(client.reset())\n",
    "#print(f\"üéÆ Available Environments: {available_environments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Loading Section\n",
    "\n",
    "### The Avengers: Specialized LoRA Models\n",
    "\n",
    "Each Avenger is a LoRA adapter trained on a specific environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Model configurations loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Avengers models\n",
    "AVENGERS_CONFIG = {\n",
    "    'cartpole': {\n",
    "        'name': 'Captain America (CartPole)',\n",
    "        'adapter_path': '/shared-docker/adapters/cartpole-lora-grpo_trained',\n",
    "        'base_model': 'unsloth/Llama-3.2-3B-Instruct'  # Assuming base model, update if different\n",
    "    },\n",
    "    'mountaincarcontinuous': {\n",
    "        'name': 'Iron Man (MountainCarContinuous)',\n",
    "        'adapter_path': '/shared-docker//adapters/mountaincart-lora-grpo_trained',\n",
    "        'base_model': 'unsloth/Llama-3.2-3B-Instruct'  # Assuming base model, update if different\n",
    "    },\n",
    "    'lunarlandercontinuous': {\n",
    "        'name': 'Thor (LunarLanderContinuous)',\n",
    "        'adapter_path': '/shared-docker//adapters/lunarlander-lora-grpo_trained',\n",
    "        'base_model': 'unsloth/Llama-3.2-3B-Instruct'  # Assuming base model, update if different\n",
    "    },\n",
    "    'bipedalwalker': {\n",
    "        'name': 'Black Widow (Bipedal Walker)',\n",
    "        'adapter_path': '/shared-docker//adapters/bipedal-walker-grpo_trained',\n",
    "        'base_model': 'unsloth/Llama-3.2-3B-Instruct'  # Assuming base model, update if different\n",
    "    }\n",
    "}\n",
    "\n",
    "# Thanos configuration\n",
    "THANOS_CONFIG = {\n",
    "    'name': 'Thanos (20B Parameter Giant)',\n",
    "    'model_name': 'unsloth/gpt-oss-20b',\n",
    "    'max_seq_length': max_seq_length  # Adjust as needed\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Model configurations loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eded23e2f034cc09d99d3dc476c4343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Loading Captain America (CartPole)...\n",
      "‚úÖ Captain America (CartPole) loaded successfully\n",
      "\n",
      "üèÜ Loading Iron Man (MountainCarContinuous)...\n",
      "‚úÖ Iron Man (MountainCarContinuous) loaded successfully\n",
      "\n",
      "üèÜ Loading Thor (LunarLanderContinuous)...\n",
      "‚úÖ Thor (LunarLanderContinuous) loaded successfully\n",
      "\n",
      "üèÜ Loading Black Widow (Bipedal Walker)...\n",
      "‚úÖ Black Widow (Bipedal Walker) loaded successfully\n",
      "\n",
      "ü¶∏ Avengers Team Ready: 4 heroes loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Avengers models (LoRA adapters)\n",
    "avengers_models = {}\n",
    "\n",
    "# Load base model\n",
    "model_base, tokenizer_base = FastLanguageModel.from_pretrained(\n",
    "            model_name=AVENGERS_CONFIG['cartpole']['base_model'],\n",
    "            load_in_4bit=False,\n",
    "            max_seq_length=max_seq_length\n",
    "        )\n",
    "\n",
    "for env_name, config in AVENGERS_CONFIG.items():\n",
    "    print(f\"\\nüèÜ Loading {config['name']}...\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        model_avenger = PeftModel.from_pretrained(model_base, config['adapter_path'])\n",
    "        \n",
    "        # Enable faster inference\n",
    "        FastLanguageModel.for_inference(model_avenger)\n",
    "        \n",
    "        avengers_models[env_name] = {\n",
    "            'model': model_avenger,\n",
    "            'tokenizer': tokenizer_base,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {config['name']} loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {config['name']}: {str(e)}\")\n",
    "        \n",
    "print(f\"\\nü¶∏ Avengers Team Ready: {len(avengers_models)} heroes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü£ Loading Thanos (20B Parameter Giant)...\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "# Load Thanos (20B model)\n",
    "print(f\"üü£ Loading {THANOS_CONFIG['name']}...\")\n",
    "\n",
    "try:\n",
    "    thanos_model, thanos_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=THANOS_CONFIG['model_name'],\n",
    "        load_in_4bit=True,\n",
    "        max_seq_length=THANOS_CONFIG['max_seq_length']\n",
    "    )\n",
    "    \n",
    "    # Enable faster inference\n",
    "    FastLanguageModel.for_inference(thanos_model)\n",
    "    \n",
    "    print(f\"‚úÖ {THANOS_CONFIG['name']} loaded successfully\")\n",
    "    print(f\"üìè Model parameters: ~{thanos_model.num_parameters():,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load {THANOS_CONFIG['name']}: {str(e)}\")\n",
    "    thanos_model = None\n",
    "    thanos_tokenizer = None\n",
    "\n",
    "print(\"\\n‚öîÔ∏è Battle preparations complete! Avengers vs Thanos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Strategy Generation System\n",
    "\n",
    "Each model must generate Python functions that implement their strategies for each environment.\n",
    "We provide structured prompts with environment details and action space requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avengers RL ‚Äî Strategy Parser\n",
    "\n",
    "WHY THIS EXISTS\n",
    "---------------\n",
    "In the Avengers RL setup, each \"Avenger\" (specialist agent) is configured with a natural-language\n",
    "`prompt` that *also* embeds a concrete Python policy function. During experiment setup, we need to:\n",
    "\n",
    "1) Extract the policy function definition for that Avenger (e.g., `def cartpole_strategy(state): ...`)\n",
    "2) Compile it in a tightly restricted environment (via `create_locked_down_function`)\n",
    "3) Hand the callable strategy back to the simulator/router\n",
    "\n",
    "This module provides two tiny helpers for that minimal workflow:\n",
    "- `_safe_compile`: hands raw source to the notebook/runtime‚Äôs locked-down compiler (assumed provided)\n",
    "- `extract_function`: finds the first `def ...strategy...` function inside the first triple-backtick block\n",
    "\n",
    "SCOPE & ASSUMPTIONS\n",
    "-------------------\n",
    "- We assume the *first* fenced code block ```...``` in the text contains the function.\n",
    "- We assume a Python function name containing the substring \"strategy\" (e.g., `*_strategy*`).\n",
    "- We strip an optional \"python\" language tag from the start of the code block.\n",
    "- We return the function **from the `def` line onward**; we do not parse/validate indentation or body.\n",
    "- This is intentionally *not* robust‚Äîlightweight by design for fast iteration in notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_function(text: str):\n",
    "    \"\"\"\n",
    "    Extract the first Python function whose `def` line contains the substring 'strategy'\n",
    "    from the first fenced code block in the given text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A string that includes exactly one strategy function wrapped in triple backticks:\n",
    "        ```\n",
    "        python\n",
    "        def some_env_strategy(state):\n",
    "            ...\n",
    "        ```\n",
    "        The 'python' language tag is optional.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str | None\n",
    "        The function source beginning at the 'def' line, or None if nothing matched.\n",
    "\n",
    "    Design Choices\n",
    "    --------------\n",
    "    - Minimal scanning (no regex, no AST) to keep it simple and fast in notebooks.\n",
    "    - Intended for well-formed prompts that we control in this project.\n",
    "    \"\"\"\n",
    "    # Assume the function is between the first pair of triple backticks\n",
    "    if text.count(\"```\") >= 2:\n",
    "        first = text.find(\"```\") + 3\n",
    "        second = text.find(\"```\", first)\n",
    "        fx = text[first:second].strip()\n",
    "\n",
    "        # Strip optional language tag like \"python\\n\"\n",
    "        if fx.startswith(\"python\\n\"):\n",
    "            fx = fx[len(\"python\\n\"):]\n",
    "\n",
    "        # Find the first 'def ...strategy...' line and return from there (inclusive)\n",
    "        i = 0\n",
    "        while True:\n",
    "            idx = fx.find(\"def\", i)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            line_end = fx.find(\"\\n\", idx)\n",
    "            line = fx[idx:] if line_end == -1 else fx[idx:line_end]\n",
    "            if \"strategy\" in line:\n",
    "                return fx[idx:].rstrip()\n",
    "            i = idx + 3\n",
    "    return None\n",
    "\n",
    "\n",
    "def _safe_compile(func_src: str):\n",
    "    \"\"\"\n",
    "    Compile a strategy function in a restricted environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func_src : str\n",
    "        The full source code of a single Python function definition starting at 'def ...'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    callable\n",
    "        A callable function object constructed by the notebook/runtime's sandboxed compiler.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - We rely on a global `create_locked_down_function` symbol provided by the notebook runtime.\n",
    "      It should evaluate/compile the given source with limited builtins and no file/network access.\n",
    "    - This indirection keeps our code agnostic of the underlying sandbox implementation.\n",
    "    \"\"\"\n",
    "    # Use the notebook's 'create_locked_down_function' if present\n",
    "    return create_locked_down_function(func_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_CONFIGS = {\n",
    "    'cartpole': {\n",
    "            'description': 'CartPole-v1: Balance a pole on a cart. The cart can move left/right, pole must stay upright.',\n",
    "            'observation_space': 'Box(4): [cart_position, cart_velocity, pole_angle, pole_angular_velocity]',\n",
    "            'action_space': 'Discrete(2): [0=left, 1=right]',\n",
    "            'reward_structure': 'Reward +1 for each timestep pole remains upright, episode ends when pole falls (>15¬∞) or 500 steps',\n",
    "            'difficulty': 'Easy - requires basic balance control',\n",
    "            'prompt': dedent(\"\"\"You are an expert CartPole player and a precise Python code generator.\n",
    "                \n",
    "                Context / How this will be used\n",
    "                - Your function will be called every environment step to control the entire episode of CartPole-v1 (OpenAI Gym/Gymnasium style).\n",
    "                - The simulator updates at ~0.02 s per step (‚âà50 Hz).\n",
    "                - The episode ends early if the pole falls or the cart goes out of bounds; otherwise it caps at the env‚Äôs max length.\n",
    "                  - Termination (approx.): |angle| > ~0.209 rad (‚âà12¬∞) or |x| > 2.4 m.\n",
    "                  - Reward is +1 per step; the goal is to survive as long as possible (ideally to the cap).\n",
    "                \n",
    "                Your objective: keep the pole upright and the cart within bounds for the longest possible duration.\n",
    "                \n",
    "                What you must write\n",
    "                - A single Python function with this exact signature (no extras):\n",
    "                    def cartpole_strategy(state):\n",
    "                - Input state is a list of 4 floats: [x, dx, angle, dangle]\n",
    "                  - x = cart position (m)\n",
    "                  - dx = cart velocity (m/s)\n",
    "                  - angle = pole angle (rad, 0 is upright; + leans right)\n",
    "                  - dangle = angular velocity (rad/s)\n",
    "                - Output: return an int action ‚Äî 0 (push left) or 1 (push right).\n",
    "                \n",
    "                Design guidance for long-horizon stability\n",
    "                - Prioritize angle correction, then angular velocity damping, and only then center the cart (x, dx) to avoid boundary terminations.\n",
    "                - Use a simple deterministic control law (e.g., a weighted linear rule with a small dead-zone/hysteresis to avoid flapping on noise).\n",
    "                - Keep it short and stateless (no memory): e.g., one or two thresholds or a sign of a weighted sum is fine.\n",
    "                - Avoid overreacting to tiny oscillations; prefer small margins rather than exact limit chasing.\n",
    "                - No stochasticity; identical inputs must produce identical outputs.\n",
    "                \n",
    "                Hard constraints\n",
    "                - Do not import, print, read/write files, use globals, randomness, or I/O.\n",
    "                - The output must be exactly one fenced code block in Python, with nothing before or after.\n",
    "                  - The first line inside the block must be: def cartpole_strategy(state):\n",
    "                  - The last line of your entire response must be the closing backticks to clearly end the program. No trailing commentary.\n",
    "                \n",
    "                Output format reminder (dummy example ‚Äî do NOT copy this logic):\n",
    "                ```\n",
    "                def cartpole_strategy(state):\n",
    "                    x, dx, ang, dang = state\n",
    "                    # return 0 or 1 using a short, deterministic rule\n",
    "                    return 1 if (0.9*ang + 0.4*dang + 0.05*x + 0.02*dx) > 0 else 0 \n",
    "                ```\n",
    "                All helper functions should be inside def cartpole_strategy. Only output the short function `strategy`.\n",
    "                \"\"\")\n",
    "                    },\n",
    "                \n",
    "    'mountaincarcontinuous': {\n",
    "                    'description': 'MountainCarContinuous-v0: Drive a car up a hill using momentum.',\n",
    "                    'observation_space': 'Box(2): [car_position (-1.2 to 0.6), car_velocity (-0.07 to 0.07)]',\n",
    "                    'action_space': 'Box(1): continuous force (-1.0 to 1.0)',\n",
    "                    'reward_structure': 'Reward = 100 * (position - (-0.5))^2 + 0.1 * velocity^2, goal position = 0.45',\n",
    "                    'difficulty': 'Medium - requires learning to build momentum in reverse',\n",
    "                    'prompt': dedent(\"\"\"You are an expert MountainCarContinuous player and a precise Python code generator.\n",
    "            \n",
    "            Context / How this will be used\n",
    "            - Your function will be called every environment step to control the entire episode of MountainCarContinuous-v0 (OpenAI Gym/Gymnasium style, continuous actions).\n",
    "            - The episode ends when the car reaches the goal (typically x ‚â• 0.45‚Äì0.5) or when the environment hits its max step limit.\n",
    "            - Reward shaping encourages reaching the goal quickly while using as little engine force as possible (penalizes large |action|).\n",
    "            \n",
    "            Your objective: reach the goal position as fast as possible by smartly building and exploiting momentum on the hills, while avoiding wasteful throttle.\n",
    "            \n",
    "            What you must write\n",
    "            - A single Python function with this exact signature (no extras):\n",
    "                def mountaincar_strategy(state):\n",
    "            - Input `state` is a list of 2 floats: [x, v]\n",
    "              - x = car position (range about [-1.2, 0.6])\n",
    "              - v = car velocity (range about [-0.07, 0.07])\n",
    "            - Output: return a **float** action in **[-1.0, 1.0]**\n",
    "              - negative = throttle left, positive = throttle right, zero ‚âà coasting.\n",
    "              - The caller will pass this scalar to the env (wrapping into a length-1 array if needed).\n",
    "            \n",
    "            Design guidance for long-horizon control\n",
    "            - MountainCarContinuous requires **momentum pumping**: generally apply force in the direction of current velocity to amplify swings, then **time the reversal** to crest the right hill.\n",
    "            - Prefer a **short, deterministic** control law (e.g., a weighted linear rule with saturating clamp to [-1, 1]).\n",
    "            - Add a **deadband/hysteresis** around v ‚âà 0 to avoid rapid action sign flips; coasting (‚âà0) can be beneficial when switching directions.\n",
    "            - Useful heuristics:\n",
    "              - If v > +Œµ, bias action right; if v < -Œµ, bias action left.\n",
    "              - When far on the left (x < -0.5), allow larger left pushes to harvest momentum.\n",
    "              - Near the final ascent on the right (x > -0.2), bias action right and reduce needless oscillation.\n",
    "              - Optionally damp action magnitude by |v| to avoid over-throttling at high speeds.\n",
    "            \n",
    "            Hard constraints\n",
    "            - Do not import, print, read/write files, use globals, randomness, or any I/O.\n",
    "            - Keep the code short and stateless; identical inputs must produce identical outputs.\n",
    "            - The output must be exactly one fenced code block in Python, with nothing before or after.\n",
    "              - The first line inside the block must be: def mountaincar_strategy(state):\n",
    "              - The last line of your entire response must be the closing backticks to clearly end the program. No trailing commentary.\n",
    "            \n",
    "            Output format reminder (dummy example ‚Äî do NOT copy this logic):\n",
    "            ```\n",
    "            def mountaincar_strategy(state):\n",
    "                x, v = state\n",
    "                return 1 * x * v # dummy example\n",
    "            ```\n",
    "            All helper functions should be inside def mountaincar_strategy. Only output the short function `strategy`.\"\"\"\n",
    "                                    )\n",
    "    },\n",
    "    'lunarlandercontinuous': {\n",
    "        'description': 'BipedalWalker-v3: Control a 2-legged robot to walk across uneven terrain without falling.',\n",
    "        'observation_space': 'Box(8): position, velocity, angle, angular velocity, leg contact sensors',\n",
    "        'action_space': 'Box(2): [main_engine_thrust (0-1), lateral_engine_force (-1 to 1)]',\n",
    "        'reward_structure': 'Complex: landing, fuel efficiency, crash penalties, movement rewards',\n",
    "        'difficulty': 'Hard - multi-dimensional continuous control',\n",
    "        'prompt': dedent(\"\"\"\n",
    "        You are an expert LunarLander (continuous control) pilot and a precise Python code generator.\n",
    "\n",
    "        Context / How this will be used\n",
    "        - Your function will be called every environment step to control the entire episode of LunarLanderContinuous-v2 (OpenAI Gym/Gymnasium, Box2D).\n",
    "        - The simulator runs at ~50 Hz. The episode ends when the lander is safely on the pad, crashes, flies off-screen, or hits the environment step cap.\n",
    "        - The reward is shaped for soft, centered landings: proximity to the pad and zeroing velocity/tilt are good; fuel use and large forces are penalized; leg contacts yield bonuses; crashing yields large negatives.\n",
    "        \n",
    "        Your objective: achieve a safe, fuel-efficient landing near the center pad by minimizing horizontal/vertical speeds and tilt while avoiding hard thrusting.\n",
    "        \n",
    "        What you must write\n",
    "        - A single Python function with this exact signature (no extras):\n",
    "            def lunarlander_strategy(state):\n",
    "        - Input `state` is a list of 8 values: [x, y, vx, vy, theta, vtheta, left_contact, right_contact]\n",
    "          - x, y       = position relative to pad center (0,0)\n",
    "          - vx, vy     = horizontal and vertical velocities\n",
    "          - theta      = lander angle (radians; 0 is upright; positive leans right)\n",
    "          - vtheta     = angular velocity\n",
    "          - left_contact, right_contact = leg contact flags (0.0 or 1.0)\n",
    "        - Output: return a length-2 sequence of floats **[main, lateral]**, each in **[-1.0, 1.0]**\n",
    "          - `main`   = main engine command (upward thrust). Negative values are treated as ‚Äúoff‚Äù; positive values increase thrust.\n",
    "          - `lateral`= side thruster command (negative pushes left, positive pushes right).\n",
    "        \n",
    "        Design guidance for long-horizon control\n",
    "        - Prioritize **vertical stabilization** (reduce `vy`) and **upright attitude** (`theta‚âà0`, `vtheta‚âà0`), then **center horizontally** (`x‚âà0`, `vx‚âà0`).\n",
    "        - Use **smooth, deterministic** control laws (e.g., weighted linear feedback with deadbands) and **clamp** outputs to [-1, 1].\n",
    "        - Add small **dead-zones/hysteresis** around zero for `vx`, `vy`, and `theta` to avoid jitter and fuel waste.\n",
    "        - Useful heuristics:\n",
    "          - Main thrust should counteract downward speed: increase with positive descent rate (`vy < 0`) and with tilt error magnitude.\n",
    "          - Lateral thruster should reduce horizontal error and velocity and help re-center the pad under the craft.\n",
    "          - Reduce thrust when either leg has contact and vertical speed is small to prevent bouncing.\n",
    "          - Cap commands gently to avoid saturating engines; prefer incremental adjustments as you approach touchdown.\n",
    "        \n",
    "        Hard constraints\n",
    "        - Do not import, print, read/write files, use globals, randomness, or any I/O.\n",
    "        - Keep the code short and stateless; identical inputs must produce identical outputs.\n",
    "        - The output must be exactly one fenced code block in Python, with nothing before or after.\n",
    "          - The first line inside the block must be: def lunarlander_strategy(state):\n",
    "          - The last line of your entire response must be the closing backticks to clearly end the program. No trailing commentary.\n",
    "        \n",
    "        Output format reminder (dummy example ‚Äî do NOT copy this logic):\n",
    "        \n",
    "        ```\n",
    "        def lunarlander_strategy(state):\n",
    "            x, y, vx, vy, th, vth, lc, rc = state\n",
    "            m = x\n",
    "            lat = y\n",
    "            return [m, lat] # example \n",
    "        ```\n",
    "        All helper functions should be inside def lunarlander_strategy. Only output the short function `strategy`.\n",
    "            \"\"\".strip()\n",
    "                        )\n",
    "                },\n",
    "    'bipedalwalker': {\n",
    "                'description': 'BipedalWalker-v3: Control a 2-legged robot to walk across uneven terrain without falling.',\n",
    "                'observation_space': (\n",
    "                    'Box(24): [hull_angle, hull_angular_velocity, vel_x, vel_y, '\n",
    "                    'joint angles & angular velocities for hips/knees (2 legs √ó 2 joints √ó 2), '\n",
    "                    'leg contact flags (2), 10 LIDAR terrain range readings]'\n",
    "                ),\n",
    "                'action_space': 'Box(4): continuous joint torques in [-1, 1] for [left_hip, left_knee, right_hip, right_knee]',\n",
    "                'reward_structure': (\n",
    "                    'Shaped: positive for forward progress and stable posture; '\n",
    "                    'penalties for excessive torque/energy and ground impacts; '\n",
    "                    'large negative on falling. Episode ends on fall or time limit.'\n",
    "                ),\n",
    "                'difficulty': 'Hard ‚Äì coordinated gait, balance, and energy management required',\n",
    "                'prompt': dedent(\"\"\"\n",
    "            Create a new short **Bipedal Walker agent strategy** using only native Python code.\n",
    "            You are given a 24-element list representing the current observation from the environment:\n",
    "            `[hull_angle, hull_angular_velocity, horizontal_speed, vertical_speed, hip1_angle, hip1_speed, hip2_angle, hip2_speed, leg1_contact, hip3_angle, hip3_speed, hip4_angle, hip4_speed, leg2_contact, lidar_1, lidar_2, lidar_3, lidar_4, lidar_5, lidar_6, lidar_7, lidar_8, lidar_9, lidar_10]`\n",
    "            \n",
    "            Output one action as a list of 4 continuous values in the range `[-1.0, 1.0]`, corresponding to the motor speeds of the walker‚Äôs four joints:\n",
    "            `[hip1, knee1, hip2, knee2]`.\n",
    "            \n",
    "            The goal is to move forward smoothly through uneven terrain without falling, keeping balance, maintaining momentum, and minimizing unnecessary torque.\n",
    "            \n",
    "            Follow these rules for better performance:\n",
    "            \n",
    "            1. **Balance First:** Keep the hull angle near zero and angular velocity small to stay upright.\n",
    "            2. **Step Rhythmically:** Alternate leg movements to create a walking rhythm.\n",
    "            3. **Gentle Force:** Avoid sudden, large torques ‚Äî use small, smooth joint movements.\n",
    "            4. **Forward Motion:** Encourage slight forward horizontal speed while avoiding high vertical speed.\n",
    "            5. **Terrain Awareness:** Use lidar readings to detect upcoming slopes and adjust the step accordingly.\n",
    "            6. **Recovery Logic:** If the hull tilts too much or a leg loses contact, adjust opposite leg torque to stabilize.\n",
    "            \n",
    "            Output your new short function in backticks using the format below:\n",
    "            \n",
    "            ```python\n",
    "            def strategy(obs) -> list[float]:\n",
    "                return [0, 0, 0, 0]  # Example\n",
    "            ```\n",
    "            \n",
    "            All helper logic should be inside `def strategy`.\n",
    "            Only output the short function `strategy`.\n",
    "            \"\"\".strip()\n",
    "                                )\n",
    "        }\n",
    "\n",
    "}\n",
    "\n",
    "# Base prompt template\n",
    "STRATEGY_PROMPT_TEMPLATE = \"\"\"\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Example usage over ENV_CONFIGS ---\n",
    "# In the Avengers RL config, each environment dictionary should include a 'prompt'\n",
    "# that embeds a fenced code block with a `*strategy*` function.\n",
    "for env in ENV_CONFIGS:\n",
    "    print(f\"Found new config for {env}\")\n",
    "\n",
    "    # Preview the beginning of the prompt for sanity\n",
    "    print(ENV_CONFIGS[env][\"prompt\"][0:100])\n",
    "\n",
    "    # Extract strategy source from the fenced code\n",
    "    func_txt = extract_python_function(ENV_CONFIGS[env][\"prompt\"])\n",
    "    print(func_txt)\n",
    "\n",
    "    # Compile in a locked/sandboxed environment\n",
    "    compiled_fn = _safe_compile(func_txt)\n",
    "    print(compiled_fn)\n",
    "\n",
    "    print(\"End of new config, function compiles OK\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate_strategy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    env_name: str,\n",
    "    model_type: str = \"specialist\",   # \"specialist\" -> deterministic; \"generalist\" -> sampled\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generate a strategy function for a given environment using a chat LLM.\n",
    "\n",
    "    - Uses the tokenizer's chat template end-to-end.\n",
    "    - Avoids double tokenization and brittle slicing.\n",
    "    - Properly sets EOS/PAD and (de)terministic sampling based on model_type.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build the prompt from your config\n",
    "    cfg = ENV_CONFIGS[env_name]\n",
    "    # Make sure STRATEGY_PROMPT_TEMPLATE has a single {} placeholder\n",
    "    user_prompt = STRATEGY_PROMPT_TEMPLATE.format(cfg[\"prompt\"])\n",
    "\n",
    "    # 2) Render chat into model-ready text once (no double tokenization)\n",
    "    chat = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    rendered = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,   # adds the assistant prefix if template expects it\n",
    "        reasoning_effort=\"low\",       # ignored by most models; harmless if unsupported\n",
    "    )\n",
    "\n",
    "    # 3) Tokenize exactly once and move to model device\n",
    "    device = getattr(model, \"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(rendered, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 4) Reproducibility & eval guards\n",
    "    model.eval()\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.manualSeed(seed)\n",
    "\n",
    "    # 5) Sampling mode\n",
    "    do_sample = (model_type != \"specialist\")  # deterministic for \"specialist\"\n",
    "    # temperature/top_p only matter if do_sample=True; set to safe defaults otherwise\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        # small helpers that often improve code quality\n",
    "        repetition_penalty=1.05 if do_sample else 1.0,\n",
    "    )\n",
    "\n",
    "    # 6) EOS/PAD safety ‚Äî important for chat models with special end tokens\n",
    "    # Collect likely EOS tokens (model-dependent; harmless if missing)\n",
    "    eos_ids = []\n",
    "    for tok in (\"eos_token_id\",):\n",
    "        tid = getattr(tokenizer, tok, None)\n",
    "        if isinstance(tid, int):\n",
    "            eos_ids.append(tid)\n",
    "    # Some chat models define additional special ends like <|eot_id|>\n",
    "    try:\n",
    "        eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        if isinstance(eot_id, int) and eot_id != tokenizer.unk_token_id:\n",
    "            eos_ids.append(eot_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if eos_ids:\n",
    "        gen_cfg.eos_token_id = eos_ids if len(eos_ids) > 1 else eos_ids[0]\n",
    "\n",
    "    # Ensure pad_token_id exists to avoid warnings on some models\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token\n",
    "    gen_kwargs = dict(generation_config=gen_cfg, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    # 7) Generate efficiently\n",
    "    start = time.time()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inputs, **gen_kwargs, return_dict_in_generate=True)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # 8) Slice only the newly generated tokens using the returned sequences\n",
    "    #    This is more robust than decoding and subtracting prompt text.\n",
    "    seq = out.sequences[0]\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = seq[prompt_len:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(f\"ü§ñ Generating strategy for {env_name} using {model_type}...\")\n",
    "    print(f\"‚è±Ô∏è Strategy generated in {elapsed:.2f}s\")\n",
    "\n",
    "    return response.strip(), elapsed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_strategy_function(code_string: str) -> Optional[callable]:\n",
    "    \"\"\"Create an executable function from code string.\"\"\"\n",
    "    try:\n",
    "        # Clean the code\n",
    "        cleaned_code = extract_python_function(code_string)\n",
    "        if not cleaned_code:\n",
    "            print(\"‚ùå Function is not found\")\n",
    "            return None\n",
    "        return _safe_compile(cleaned_code)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create function: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"üéØ Strategy generation system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate strategies for all environments\n",
    "strategies = {\n",
    "    'avengers': {},\n",
    "    'thanos': {}\n",
    "}\n",
    "\n",
    "generation_stats = {\n",
    "    'avengers': {},\n",
    "    'thanos': {}\n",
    "}\n",
    "\n",
    "# Generate Avengers strategies (specialists for their environments)\n",
    "for env_name in ENV_CONFIGS.keys():\n",
    "    if env_name in avengers_models:\n",
    "        model_data = avengers_models[env_name]\n",
    "        \n",
    "        response, gen_time = generate_strategy(\n",
    "            model_data['model'], \n",
    "            model_data['tokenizer'], \n",
    "            env_name, \n",
    "            model_type='avenger',\n",
    "            max_new_tokens=1024\n",
    "        )\n",
    "        \n",
    "        function = create_strategy_function(response)\n",
    "        \n",
    "        strategies['avengers'][env_name] = {\n",
    "            'code': response,\n",
    "            'function': function,\n",
    "            'hero': AVENGERS_CONFIG[env_name]['name']\n",
    "        }\n",
    "        \n",
    "        generation_stats['avengers'][env_name] = gen_time\n",
    "\n",
    "# Generate Thanos strategies (generalist for all environments)\n",
    "if thanos_model is not None:\n",
    "    \n",
    "    for env_name in ENV_CONFIGS.keys():\n",
    "        response, gen_time = generate_strategy(\n",
    "            thanos_model, \n",
    "            thanos_tokenizer, \n",
    "            env_name, \n",
    "            model_type='thanos',\n",
    "            max_new_tokens=2048\n",
    "        )\n",
    "        \n",
    "        function = create_strategy_function(response)\n",
    "        \n",
    "        strategies['thanos'][env_name] = {\n",
    "            'code': response,\n",
    "            'function': function\n",
    "        }\n",
    "        \n",
    "        generation_stats['thanos'][env_name] = gen_time\n",
    "\n",
    "print(\"\\nüìä Strategy Generation Complete!\")\n",
    "print(f\"ü¶∏ Avengers: {len(strategies['avengers'])} strategies\")\n",
    "print(f\"üü£ Thanos: {len(strategies['thanos'])} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluation Framework\n",
    "\n",
    "Time to put our heroes to the test! We'll run each strategy across multiple episodes and collect comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Framework for Sequential Environment Strategies.\n",
    "\n",
    "Adapted version of evaluate_strategy for sequential environments that steps through\n",
    "cartpole, mountaincar, lunarlander, and bipedalwalker phases in sequence while\n",
    "remembering the last state of each environment.\n",
    "\n",
    "The strategy_func should be a dictionary mapping environment names to strategy functions:\n",
    "strategies = {\n",
    "    'cartpole': lambda obs: 0,  # discrete action {0,1}\n",
    "    'mountaincarcontinuous': lambda obs: 0.5,  # continuous action [-1.0, 1.0]\n",
    "    'lunarlandercontinuous': lambda obs: [0.0, -0.5],  # [main_thrust, lateral_thrust]\n",
    "    'bipedalwalker': lambda obs: [0.0, 0.0, 0.0, 0.0]  # [hip1, knee1, hip2, knee2]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "from envs.sequential_environment.models import SequentialAction\n",
    "\n",
    "\n",
    "def get_action_from_strategies(strategies_dict: Dict[str, Any], action: SequentialAction, phase: str, sub_obs: list) -> SequentialAction:\n",
    "    \"\"\"\n",
    "    Route the phase to the appropriate strategy function and set the action.\n",
    "\n",
    "    Args:\n",
    "        strategies_dict: Dictionary mapping env names to strategy functions\n",
    "        action: SequentialAction to modify\n",
    "        phase: Current active phase ('cartpole', 'mountaincar', 'lunarlander', 'bipedalwalker')\n",
    "        sub_obs: Sub-observation for the current phase\n",
    "\n",
    "    Returns:\n",
    "        Modified SequentialAction with the appropriate action set for the active phase\n",
    "    \"\"\"\n",
    "    # Map phase names to dictionary keys\n",
    "    phase_mapping = {\n",
    "        'cartpole': 'cartpole',\n",
    "        'mountaincar': 'mountaincarcontinuous',\n",
    "        'lunarlander': 'lunarlandercontinuous',\n",
    "        'bipedalwalker': 'bipedalwalker'\n",
    "    }\n",
    "\n",
    "    env_key = phase_mapping.get(phase)\n",
    "    if env_key not in strategies_dict:\n",
    "        print(f\"‚ö†Ô∏è  No strategy found for phase '{phase}' (env_key: '{env_key}')\")\n",
    "        return action\n",
    "    strategy_func = strategies_dict[env_key][\"function\"]\n",
    "    # print(strategies_dict[env_key])\n",
    "    try:\n",
    "        # Get action from strategy function\n",
    "        result = strategy_func(sub_obs)\n",
    "        # print(result)\n",
    "        # Set the action for the active phase\n",
    "        if phase == 'cartpole':\n",
    "            action.cartpole_action = int(result) if isinstance(result, (int, float, np.integer, np.floating)) else 0\n",
    "            action.cartpole_action = max(0, min(1, action.cartpole_action))  # Discrete {0,1}\n",
    "        elif phase == 'mountaincar':\n",
    "            if isinstance(result, (int, float, np.integer, np.floating)):\n",
    "                action.mountaincar_action = float(np.clip(result, -1.0, 1.0))\n",
    "            else:\n",
    "                action.mountaincar_action = 0.0  # Default\n",
    "        elif phase == 'lunarlander':\n",
    "            if isinstance(result, (list, np.ndarray)):\n",
    "                action.lunarlander_action = [float(x) for x in result[:2]]  # Take first 2 elements\n",
    "                action.lunarlander_action = [np.clip(x, -1.0, 1.0) for x in action.lunarlander_action]\n",
    "            else:\n",
    "                action.lunarlander_action = [0.0, 0.0]  # Default\n",
    "        elif phase == 'bipedalwalker':\n",
    "            if isinstance(result, (list, np.ndarray)):\n",
    "                # Take first 4 elements, clip to [-1.0, 1.0] each\n",
    "                action.bipedalwalker_action = [float(np.clip(x, -1.0, 1.0)) for x in result[:4]]\n",
    "                # Ensure exactly 4 actions\n",
    "                while len(action.bipedalwalker_action) < 4:\n",
    "                    action.bipedalwalker_action.append(0.0)\n",
    "                # print(action.bipedalwalker_action)\n",
    "            else:\n",
    "                action.bipedalwalker_action = [0.0, 0.0, 0.0, 0.0]  # Default\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Strategy error in phase {phase}: {e}\")\n",
    "        # Set defaults for the active phase\n",
    "        if phase == 'cartpole':\n",
    "            action.cartpole_action = 0\n",
    "        elif phase == 'mountaincar':\n",
    "            action.mountaincar_action = 0.0\n",
    "        elif phase == 'lunarlander':\n",
    "            action.lunarlander_action = [0.0, 0.0]\n",
    "        elif phase == 'bipedalwalker':\n",
    "            action.bipedalwalker_action = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def evaluate_sequential_strategy(client, strategy_func: Dict[str, Any], agent_name, max_steps: int = 10000, num_episodes: int = 5) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a strategy set across multiple sequential environment episodes.\n",
    "\n",
    "    In the sequential environment, phases are executed in random order:\n",
    "    cartpole ‚Üí mountaincar ‚Üí lunarlander ‚Üí bipedalwalker\n",
    "    The environment remembers the last state of each sub-environment.\n",
    "\n",
    "    Args:\n",
    "        client: Sequential environment client\n",
    "        env_name: Environment name (should be 'sequential')\n",
    "        strategy_func: Dictionary mapping env names to strategy functions:\n",
    "            {\n",
    "                'cartpole': lambda obs: 0,           # discrete {0,1}\n",
    "                'mountaincarcontinuous': lambda obs: 0.5,  # continuous [-1.0, 1.0]\n",
    "                'lunarlandercontinuous': lambda obs: [0.0, -0.5],  # [main_thrust, lateral_thrust]\n",
    "                'bipedalwalker': lambda obs: [0.0, 0.0, 0.0, 0.0]  # [hip1, knee1, hip2, knee2]\n",
    "            }\n",
    "        max_steps: Maximum steps per episode\n",
    "        num_episodes: Number of episodes to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\n",
    "        'episodes': [],\n",
    "        'total_reward': 0,\n",
    "        'avg_reward': 0,\n",
    "        'max_reward': -float('inf'),\n",
    "        'min_reward': float('inf'),\n",
    "        'total_steps': 0,\n",
    "        'avg_steps': 0,\n",
    "        'completion_rate': 0,  # Fraction of episodes where all phases completed\n",
    "        'success_episodes': 0,  # Episodes where all phases completed\n",
    "        'execution_times': [],\n",
    "        'total_execution_time': 0,\n",
    "        # Phase-specific metrics\n",
    "        'cartpole_success_rate': 0,\n",
    "        'mountaincar_success_rate': 0,\n",
    "        'lunarlander_success_rate': 0,\n",
    "        'bipedalwalker_success_rate': 0,\n",
    "        'overall_success_rate': 0,  # All phases completed\n",
    "        # Per-phase aggregated metrics\n",
    "        'total_phase_steps': {'cartpole': 0, 'mountaincar': 0, 'lunarlander': 0, 'bipedalwalker': 0},\n",
    "        'total_phase_rewards': {'cartpole': 0.0, 'mountaincar': 0.0, 'lunarlander': 0.0, 'bipedalwalker': 0.0}\n",
    "    }\n",
    "\n",
    "    print(f\"üß™ Testing {agent_name}- Running {num_episodes} sequential episodes...\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "            # Start episode - sequential environment maintains state across episodes\n",
    "            # Get initial observation\n",
    "            obs = client.reset().observation\n",
    "            episode_id = episode\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            episode_execution_time = 0\n",
    "            trajectory = []\n",
    "\n",
    "            # Track phase-specific metrics for this episode\n",
    "            phase_completion = {\n",
    "                'cartpole': False,\n",
    "                'mountaincar': False,\n",
    "                'lunarlander': False,\n",
    "                'bipedalwalker': False\n",
    "            }\n",
    "            phase_rewards = {\n",
    "                'cartpole': 0,\n",
    "                'mountaincar': 0,\n",
    "                'lunarlander': 0,\n",
    "                'bipedalwalker': 0\n",
    "            }\n",
    "\n",
    "            while episode_steps < max_steps and obs is not None and not obs.done:\n",
    "                current_phase = obs.phase\n",
    "                sub_observation = obs.sub_observation\n",
    "\n",
    "                # Create SequentialAction with defaults\n",
    "                action = SequentialAction()\n",
    "                action.cartpole_action = 0\n",
    "                action.mountaincar_action = 0.0\n",
    "                action.lunarlander_action = [0.0, 0.0]\n",
    "                action.bipedalwalker_action = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "                # Get action from strategy using our wrapper function (similar to test_sequential_strategies.py)\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    action = get_action_from_strategies(strategy_func, action, current_phase, sub_observation)\n",
    "                    # print(action)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Strategy error in episode {episode + 1}, phase {current_phase}: {e}\")\n",
    "                    # action already has defaults set above\n",
    "                execution_time = time.time() - start_time\n",
    "\n",
    "                obs = client.step(action).observation\n",
    "\n",
    "                reward = obs.reward\n",
    "                done = obs.done\n",
    "\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                episode_execution_time += execution_time\n",
    "\n",
    "                # Get current state to check phase completion\n",
    "                state = obs\n",
    "\n",
    "\n",
    "                if(current_phase == \"cartpole\"):\n",
    "                    phase_rewards['cartpole'] += reward\n",
    "                elif current_phase == \"mountaincar\":\n",
    "                    phase_rewards['mountaincar'] += reward\n",
    "                elif current_phase == \"lunarlander\":\n",
    "                    phase_rewards['lunarlander'] += reward\n",
    "                elif current_phase == \"bipedalwalker\":\n",
    "                    phase_rewards['bipedalwalker'] += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Record phase completion status\n",
    "            all_phases_complete = all(phase_completion.values())\n",
    "\n",
    "            # Record episode results\n",
    "            episode_data = {\n",
    "                'episode': episode + 1,\n",
    "                'reward': episode_reward,\n",
    "                'steps': episode_steps,\n",
    "                'success': all_phases_complete,  # All phases completed\n",
    "                'execution_time': episode_execution_time,\n",
    "                'trajectory_length': len(trajectory),\n",
    "                'phase_completion': phase_completion,\n",
    "                'cartpole_done': phase_rewards['cartpole'],\n",
    "                'mountaincar_done': phase_rewards['mountaincar'],\n",
    "                'lunarlander_done': phase_rewards['lunarlander'],\n",
    "                'bipedalwalker_done': phase_rewards['bipedalwalker'],\n",
    "            }\n",
    "\n",
    "            results['episodes'].append(episode_data)\n",
    "            results['total_reward'] += episode_reward\n",
    "            results['max_reward'] = max(results['max_reward'], episode_reward)\n",
    "            results['min_reward'] = min(results['min_reward'], episode_reward)\n",
    "            results['total_steps'] += episode_steps\n",
    "            results['execution_times'].append(episode_execution_time)\n",
    "            results['total_execution_time'] += episode_execution_time\n",
    "\n",
    "            if all_phases_complete:\n",
    "                results['success_episodes'] += 1\n",
    "\n",
    "            # Print episode summary\n",
    "            phase_status = \"\".join([\n",
    "                \"üü¢\" if phase_completion['cartpole'] else \"üî¥\",  # Cartpole\n",
    "                \"üü¢\" if phase_completion['mountaincar'] else \"üî¥\",  # MountainCar\n",
    "                \"üü¢\" if phase_completion['lunarlander'] else \"üî¥\",  # LunarLander\n",
    "                \"üü¢\" if phase_completion['bipedalwalker'] else \"üî¥\"   # BipedalWalker\n",
    "            ])\n",
    "\n",
    "            print(f\"  üìà Episode {episode + 1}: Reward={episode_reward:.1f}, Steps={episode_steps}, \"\n",
    "                  f\"Phases={phase_status}, Success={all_phases_complete}\")\n",
    "\n",
    "\n",
    "    # Calculate aggregates\n",
    "    num_completed_episodes = len(results['episodes'])\n",
    "    if num_completed_episodes > 0:\n",
    "        results['avg_reward'] = results['total_reward'] / num_completed_episodes\n",
    "        results['avg_steps'] = results['total_steps'] / num_completed_episodes\n",
    "        results['completion_rate'] = results['success_episodes'] / num_completed_episodes\n",
    "        results['avg_execution_time'] = results['total_execution_time'] / num_completed_episodes\n",
    "\n",
    "        # Calculate phase-specific success rates\n",
    "        results['cartpole_success_rate'] = sum(ep['cartpole_done'] for ep in results['episodes'] if ep['cartpole_done']) / num_completed_episodes\n",
    "        results['mountaincar_success_rate'] = sum(ep['mountaincar_done'] for ep in results['episodes'] if ep['mountaincar_done']) / num_completed_episodes\n",
    "        results['lunarlander_success_rate'] = sum(ep['lunarlander_done'] for ep in results['episodes'] if ep['lunarlander_done']) / num_completed_episodes\n",
    "        results['bipedalwalker_success_rate'] = sum(ep['bipedalwalker_done'] for ep in results['episodes'] if ep['bipedalwalker_done']) / num_completed_episodes\n",
    "        results['overall_finish_rate'] = results['completion_rate']  # Alias for consistency\n",
    "\n",
    "\n",
    "    print(f\"üìä Results {agent_name}: Avg Reward={results['avg_reward']:.1f}, Success Rate={results['completion_rate']:.1%}\")\n",
    "    print(f\"   Phase Reward: Cartpole={results['cartpole_success_rate']}, \"\n",
    "          f\"MountainCar={results['mountaincar_success_rate']}, \"\n",
    "          f\"LunarLander={results['lunarlander_success_rate']}, \"\n",
    "          f\"BipedalWalker={results['bipedalwalker_success_rate']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"üéØ Sequential Evaluation framework ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers_output = evaluate_sequential_strategy(client, strategies['avengers'],'Avengers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thanos_output = evaluate_sequential_strategy(client, strategies['thanos'], 'Thanos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Conclusion\n",
    "\n",
    "This notebook has pitted specialized LoRA-trained Avengers against Thanos, a massive 20B parameter model, in a battle to determine:\n",
    "\n",
    "**Can specialization + coordination outperform brute size?**\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Specialization Works**: Individual Avengers excel at their trained environments\n",
    "2. **Efficiency Matters**: Smaller models generate strategies faster and can be more efficient\n",
    "3. **Coordination Potential**: A team approach could leverage multiple specialists\n",
    "4. **Task Fit**: Results depend on how well the model's scale matches the task complexity\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "- **Mixture of Experts**: Combine multiple specialists with a coordinator\n",
    "- **Meta-Learning**: Train models that can adapt to new environments quickly\n",
    "- **Hybrid Approaches**: Use both large and small models in complementary ways\n",
    "\n",
    "The lesson from this showdown? **Both specialization and scale have their place** - the key is knowing when to use each approach!\n",
    "\n",
    "--The Avengers, despite their specialization, lost this round.** While Iron Man showed glimmers of brilliance with his MountainCar expertise, Thanos's sheer computational power overwhelmed the team despite the massive 8B vs 20B parameter gap. However, this is not a depressing conclusion but a motivational one - the Avengers need to regroup with:\n",
    "\n",
    "1. **Stronger foundation layers** - Our current 8B LoRA adapters simply cannot challenge the 20B giant\n",
    "\n",
    "2. **Deeper, more specialized training** - The LoRA adapters need extended, focused GRPO training\n",
    "\n",
    "3. **Strategic coordination** - Our current team lacks the orchestration layer to compete effectively\n",
    "\n",
    "4. **Model scale advancement** - We need to either increase base model size or improve fine-tuning efficiency\n",
    "\n",
    "ü¶∏‚ôÄÔ∏è **Remember Avengers fans - this battle showed the hero team's potential, but Thanos came too strong! Prepare for Round 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
