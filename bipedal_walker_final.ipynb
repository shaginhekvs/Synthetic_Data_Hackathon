{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c4828c",
   "metadata": {},
   "source": [
    "# ðŸ§  Bipedal Walker Strategy Fine-Tuning with Llama-3.2-3B\n",
    "\n",
    "This notebook sets up a LoRA-fine-tunable Llama model using `unsloth` to generate\n",
    "and evaluate **reinforcement learning strategies** for the BipedalWalker environment.\n",
    "\n",
    "It includes:\n",
    "- Model + tokenizer loading  \n",
    "- Gym environment integration  \n",
    "- Reward evaluation functions  \n",
    "- GRPO training setup  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0d64e",
   "metadata": {},
   "source": [
    "## 1. Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541872b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Radeon Graphics. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+rocm6.4. ROCm Toolkit: 6.4.43482-0f2d60242. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "BASE_ID = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model hyperparameters\n",
    "max_seq_length = 2048\n",
    "lora_rank = 128\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_ID,\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca3123",
   "metadata": {},
   "source": [
    "## 2. Apply LoRA Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1708d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d3fae",
   "metadata": {},
   "source": [
    "## 3. Gym Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a40fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reset: StepResult(observation=GymObservation(done=False, reward=None, metadata={}, state=[0.002747555961832404, -2.382378625043202e-06, 0.0001853278954513371, -0.01599998027086258, 0.09211806952953339, -0.0002445672871544957, 0.8601776361465454, 0.0015840515261515975, 1.0, 0.032523151487112045, -0.0002445560530759394, 0.8537275195121765, 0.0001730085350573063, 1.0, 0.44081392884254456, 0.44582003355026245, 0.4614226818084717, 0.48955008387565613, 0.5341026782989502, 0.6024609208106995, 0.7091487646102905, 0.885931670665741, 1.0, 1.0], legal_actions={'low': [-1.0, -1.0, -1.0, -1.0], 'high': [1.0, 1.0, 1.0, 1.0]}, episode_length=0, total_reward=0.0, frame=None), reward=None, done=False)\n",
      "Step result: StepResult(observation=GymObservation(done=False, reward=-0.1063923373310528, metadata={}, state=[-0.008335273712873459, -0.006122593302279711, -0.00252687931060791, 0.0007477727485820651, 0.43431541323661804, 0.010125350207090378, 0.12396615743637085, 0.04638111591339111, 1.0, 0.33763569593429565, 0.040201008319854736, 0.12468528747558594, 0.006310367491096258, 1.0, 0.4504973292350769, 0.4556134045124054, 0.4715588092803955, 0.500304102897644, 0.5458353757858276, 0.6156952381134033, 0.724726676940918, 0.9053930044174194, 1.0, 1.0], legal_actions={'low': [-1.0, -1.0, -1.0, -1.0], 'high': [1.0, 1.0, 1.0, 1.0]}, episode_length=1, total_reward=-0.1063923373310528, frame=None), reward=-0.1063923373310528, done=False)\n"
     ]
    }
   ],
   "source": [
    "from envs.gym_environment import GymEnvironment, GymAction\n",
    "\n",
    "base_url = \"http://localhost:9000\"\n",
    "request_timeout_s = 1000\n",
    "\n",
    "env = GymEnvironment(base_url=base_url, request_timeout_s=request_timeout_s)\n",
    "state = env.reset()\n",
    "print(\"Environment reset:\", state)\n",
    "\n",
    "# Take one step to verify environment\n",
    "state = env.step(GymAction(action=[0.1] * 4))\n",
    "print(\"Step result:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074daf1f",
   "metadata": {},
   "source": [
    "## 4. Strategy Execution Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c711c296",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from unsloth import execute_with_time_limit\n",
    "\n",
    "def _execute_strategy(strategy: Callable, env: GymEnvironment, return_frames=False):\n",
    "    \"\"\"Execute a given strategy function within the gym environment.\"\"\"\n",
    "    if not callable(strategy):\n",
    "        raise ValueError(\"Unvalid function\")\n",
    "    actions, observations, frames = [], [], []\n",
    "    obs = env.reset().observation.state\n",
    "    total, steps, done = 0.0, 0, False\n",
    "\n",
    "    while not done:\n",
    "        a = strategy(obs)\n",
    "        observations.append(obs)\n",
    "        actions.append(a)\n",
    "        step_result = env.step(GymAction(action=a, return_frame=return_frames))\n",
    "        total += step_result.observation.reward\n",
    "        done = step_result.done\n",
    "        obs = step_result.observation.state\n",
    "        steps += 1\n",
    "        if return_frames:\n",
    "            frames.append(step_result.observation.frame)\n",
    "    print(f\"Ran strategy for {steps} steps -> Scored {total}\")\n",
    "    return steps, total, frames\n",
    "\n",
    "@execute_with_time_limit(5)\n",
    "def execute_strategy(strategy: Callable, env, return_frames=False):\n",
    "    \"\"\"Run a strategy with timeout protection.\"\"\"\n",
    "    return _execute_strategy(strategy, env, return_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060f3bd",
   "metadata": {},
   "source": [
    "## 5. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5bfc2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def extract_python_code(md_text: str) -> list[str]:\n",
    "    \"\"\"Extract all Python code blocks from markdown text.\"\"\"\n",
    "    pattern = r\"```(?:python)?\\s*([\\s\\S]*?)\\s*```\"\n",
    "    return [block.strip() for block in re.findall(pattern, md_text, re.IGNORECASE) if block.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baefd05",
   "metadata": {},
   "source": [
    "## 6. User Prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc9bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create a new short **Bipedal Walker agent strategy** using only native Python code.\n",
      "You are given a 24-element list representing the current observation from the environment:\n",
      "`[hull_angle, hull_angular_velocity, horizontal_speed, vertical_speed, hip1_angle, hip1_speed, hip2_angle, hip2_speed, leg1_contact, hip3_angle, hip3_speed, hip4_angle, hip4_speed, leg2_contact, lidar_1, lidar_2, lidar_3, lidar_4, lidar_5, lidar_6, lidar_7, lidar_8, lidar_9, lidar_10]`\n",
      "\n",
      "Output one action as a list of 4 continuous values in the range `[-1.0, 1.0]`, corresponding to the motor speeds of the walkerâ€™s four joints:\n",
      "`[hip1, knee1, hip2, knee2]`.\n",
      "\n",
      "The goal is to move forward smoothly through uneven terrain without falling, keeping balance, maintaining momentum, and minimizing unnecessary torque.\n",
      "\n",
      "Follow these rules for better performance:\n",
      "\n",
      "1. **Balance First:** Keep the hull angle near zero and angular velocity small to stay upright.\n",
      "2. **Step Rhythmically:** Alternate leg movements to create a walking rhythm.\n",
      "3. **Gentle Force:** Avoid sudden, large torques â€” use small, smooth joint movements.\n",
      "4. **Forward Motion:** Encourage slight forward horizontal speed while avoiding high vertical speed.\n",
      "5. **Terrain Awareness:** Use lidar readings to detect upcoming slopes and adjust the step accordingly.\n",
      "6. **Recovery Logic:** If the hull tilts too much or a leg loses contact, adjust opposite leg torque to stabilize.\n",
      "\n",
      "Output your new short function in backticks using the format below:\n",
      "\n",
      "```python\n",
      "def strategy(obs) -> list[float]:\n",
      "    return [0, 0, 0, 0]  # Example\n",
      "```\n",
      "\n",
      "All helper logic should be inside `def strategy`.\n",
      "Only output the short function `strategy`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "USER_PROMPT = \"\"\"\n",
    "Create a new short **Bipedal Walker agent strategy** using only native Python code.\n",
    "You are given a 24-element list representing the current observation from the environment:\n",
    "`[hull_angle, hull_angular_velocity, horizontal_speed, vertical_speed, hip1_angle, hip1_speed, hip2_angle, hip2_speed, leg1_contact, hip3_angle, hip3_speed, hip4_angle, hip4_speed, leg2_contact, lidar_1, lidar_2, lidar_3, lidar_4, lidar_5, lidar_6, lidar_7, lidar_8, lidar_9, lidar_10]`\n",
    "\n",
    "Output one action as a list of 4 continuous values in the range `[-1.0, 1.0]`, corresponding to the motor speeds of the walkerâ€™s four joints:\n",
    "`[hip1, knee1, hip2, knee2]`.\n",
    "\n",
    "The goal is to move forward smoothly through uneven terrain without falling, keeping balance, maintaining momentum, and minimizing unnecessary torque.\n",
    "\n",
    "Follow these rules for better performance:\n",
    "\n",
    "1. **Balance First:** Keep the hull angle near zero and angular velocity small to stay upright.\n",
    "2. **Step Rhythmically:** Alternate leg movements to create a walking rhythm.\n",
    "3. **Gentle Force:** Avoid sudden, large torques â€” use small, smooth joint movements.\n",
    "4. **Forward Motion:** Encourage slight forward horizontal speed while avoiding high vertical speed.\n",
    "5. **Terrain Awareness:** Use lidar readings to detect upcoming slopes and adjust the step accordingly.\n",
    "6. **Recovery Logic:** If the hull tilts too much or a leg loses contact, adjust opposite leg torque to stabilize.\n",
    "\n",
    "Output your new short function in backticks using the format below:\n",
    "\n",
    "```python\n",
    "def strategy(obs) -> list[float]:\n",
    "    return [0, 0, 0, 0]  # Example\n",
    "```\n",
    "\n",
    "All helper logic should be inside `def strategy`.\n",
    "Only output the short function `strategy`.\n",
    "\"\"\"\n",
    "\n",
    "print(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a8ed1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61850c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list([{\"prompt\": [{\"role\": \"user\", \"content\": USER_PROMPT}]}] * 1000)\n",
    "maximum_length = len(tokenizer.apply_chat_template(dataset[0][\"prompt\"], add_generation_prompt=True))\n",
    "print(\"Max prompt length:\", maximum_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a78ee6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Strategy evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b02198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_strategy(model, tokenizer, prompt, max_new_tokens=512):\n",
    "    \"\"\"Generate strategy function using the trained model.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    formatted = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=1.0,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    new_tokens = out[0][formatted.shape[-1]:]\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bd1db",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Evaluation and Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba74d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from unsloth import check_python_modules, create_locked_down_function\n",
    "\n",
    "\n",
    "def score_function(steps, reward):\n",
    "    \"\"\"Compute a simple score from steps and reward.\"\"\"\n",
    "    return max(0, (120 + reward) * 2 + (1 / steps) * 10)\n",
    "\n",
    "def _run_responses(responses):\n",
    "    \"\"\"Safely evaluate generated strategies in the environment.\"\"\"\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        function_list = extract_python_code(response)\n",
    "        if not function_list:\n",
    "            scores.append(-5)\n",
    "            continue\n",
    "\n",
    "        function = function_list[-1]\n",
    "        ok, info = check_python_modules(function)\n",
    "        if \"error\" in info:\n",
    "            scores.append(-5)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            new_strategy = create_locked_down_function(function)\n",
    "            steps, reward, _ = execute_strategy(new_strategy, env)\n",
    "            scores.append(score_function(steps, reward))\n",
    "        except TimeoutError:\n",
    "            scores.append(-2.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute strategy: {e}\")\n",
    "            scores.append(-3.0)\n",
    "    return scores\n",
    "def strategy_succeeds(completions, **kwargs):\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    return _run_responses(responses)\n",
    "\n",
    "PRINTER = 0\n",
    "def function_works(completions, **kwargs):\n",
    "    global PRINTER\n",
    "    scores = []\n",
    "    # Generate a random game board with seed\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    for resp in responses:\n",
    "        if PRINTER % 5 == 0:\n",
    "            printed = True\n",
    "            print(\"SNIPPET: \"+resp[:100]+\"\\n--------------\\n\")\n",
    "        PRINTER += 1\n",
    "\n",
    "        score = 0\n",
    "        function = extract_python_code(resp)\n",
    "        if function:\n",
    "            ok, info = check_python_modules(function[-1])\n",
    "            print(function[-1]+\"\\n--------------\\n\")\n",
    "        if not function or \"error\" in info:\n",
    "            score = -2.0\n",
    "        else:\n",
    "            try:\n",
    "                new_strategy = create_locked_down_function(function[-1])\n",
    "                score = 2.0\n",
    "            except Exception as e:\n",
    "                score = -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2368f-a8c5-40ab-bfb3-9f2148a4a2f3",
   "metadata": {},
   "source": [
    "## 10. Test Base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cd6283-1df4-4973-b625-bfc816c181de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "```python\n",
      "def strategy(obs):\n",
      "    hull_angle = obs[0]\n",
      "    angular_velocity = obs[1]\n",
      "    horizontal_speed = obs[2]\n",
      "    vertical_speed = obs[3]\n",
      "    hip1_angle = obs[4]\n",
      "    hip1_speed = obs[5]\n",
      "    hip2_angle = obs[6]\n",
      "    hip2_speed = obs[7]\n",
      "    leg1_contact = obs[8]\n",
      "    hip3_angle = obs[9]\n",
      "    hip3_speed = obs[10]\n",
      "    hip4_angle = obs[11]\n",
      "    hip4_speed = obs[12]\n",
      "    leg2_contact = obs[13]\n",
      "    lidar_1 = obs[14]\n",
      "    lidar_2 = obs[15]\n",
      "    lidar_3 = obs[16]\n",
      "    lidar_4 = obs[17]\n",
      "    lidar_5 = obs[18]\n",
      "    lidar_6 = obs[19]\n",
      "    lidar_7 = obs[20]\n",
      "    lidar_8 = obs[21]\n",
      "    lidar_9 = obs[22]\n",
      "    lidar_10 = obs[23]\n",
      "\n",
      "    # Balance First\n",
      "    if abs(hull_angle) > 0.5:\n",
      "        hip1_speed = max(-abs(hip1_speed), 0)\n",
      "        hip2_speed = max(-abs(hip2_speed), 0)\n",
      "        hip3_speed = max(-abs(hip3_speed), 0)\n",
      "        hip4_speed = max(-abs(hip4_speed), 0)\n",
      "\n",
      "    # Step Rhythmically\n",
      "    if leg1_contact:\n",
      "        hip2_speed = -max(hip2_speed, 0)\n",
      "    if not leg1_contact:\n",
      "        hip1_speed = -max(hip1_speed, 0)\n",
      "\n",
      "    # Gentle Force\n",
      "    hip1_speed = max(min(hip1_speed, 0.5), -0.5)\n",
      "    hip2_speed = max(min(hip2_speed, 0.5), -0.5)\n",
      "\n",
      "    # Forward Motion\n",
      "    if horizontal_speed < 0:\n",
      "        hip1_speed = max(hip1_speed, 0.5)\n",
      "        hip2_speed = max(hip2_speed, 0.5)\n",
      "\n",
      "    # Terrain Awareness\n",
      "    if lidar_1 > 0.5:\n",
      "        hip2_speed = -max(hip2_speed, 0.5)\n",
      "    if lidar_2 > 0.5:\n",
      "        hip1_speed = -max(hip1_speed, 0.5)\n",
      "\n",
      "    # Recovery Logic\n",
      "    if lidar_1 > 0.7:\n",
      "        hip1_speed = 0\n",
      "    if lidar_2 > 0.7:\n",
      "        hip2_speed = 0\n",
      "    if lidar_3 > 0.7:\n",
      "        hip3_speed = 0\n",
      "    if lidar_4 > 0.7:\n",
      "        hip4_speed = 0\n",
      "\n",
      "    return [hip1_speed, hip2_speed, hip3_speed, hip4_speed]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generated_function = generate_strategy(model, tokenizer, [{\"role\": \"user\", \"content\": USER_PROMPT}],1024) \n",
    "print(generated_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cde912-8a62-4a1b-8f01-f777b83f469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to execute strategy: 500 Server Error: Internal Server Error for url: http://localhost:9000/step\n",
      "Strategy reward:  [-3.0]\n",
      "SNIPPET: assistant\n",
      "\n",
      "```python\n",
      "def strategy(obs):\n",
      "    hull_angle = obs[0]\n",
      "    angular_velocity = obs[1]\n",
      "    ho\n",
      "--------------\n",
      "\n",
      "def strategy(obs):\n",
      "    hull_angle = obs[0]\n",
      "    angular_velocity = obs[1]\n",
      "    horizontal_speed = obs[2]\n",
      "    vertical_speed = obs[3]\n",
      "    hip1_angle = obs[4]\n",
      "    hip1_speed = obs[5]\n",
      "    hip2_angle = obs[6]\n",
      "    hip2_speed = obs[7]\n",
      "    leg1_contact = obs[8]\n",
      "    hip3_angle = obs[9]\n",
      "    hip3_speed = obs[10]\n",
      "    hip4_angle = obs[11]\n",
      "    hip4_speed = obs[12]\n",
      "    leg2_contact = obs[13]\n",
      "    lidar_1 = obs[14]\n",
      "    lidar_2 = obs[15]\n",
      "    lidar_3 = obs[16]\n",
      "    lidar_4 = obs[17]\n",
      "    lidar_5 = obs[18]\n",
      "    lidar_6 = obs[19]\n",
      "    lidar_7 = obs[20]\n",
      "    lidar_8 = obs[21]\n",
      "    lidar_9 = obs[22]\n",
      "    lidar_10 = obs[23]\n",
      "\n",
      "    # Balance First\n",
      "    if abs(hull_angle) > 0.5:\n",
      "        hip1_speed = max(-abs(hip1_speed), 0)\n",
      "        hip2_speed = max(-abs(hip2_speed), 0)\n",
      "        hip3_speed = max(-abs(hip3_speed), 0)\n",
      "        hip4_speed = max(-abs(hip4_speed), 0)\n",
      "\n",
      "    # Step Rhythmically\n",
      "    if leg1_contact:\n",
      "        hip2_speed = -max(hip2_speed, 0)\n",
      "    if not leg1_contact:\n",
      "        hip1_speed = -max(hip1_speed, 0)\n",
      "\n",
      "    # Gentle Force\n",
      "    hip1_speed = max(min(hip1_speed, 0.5), -0.5)\n",
      "    hip2_speed = max(min(hip2_speed, 0.5), -0.5)\n",
      "\n",
      "    # Forward Motion\n",
      "    if horizontal_speed < 0:\n",
      "        hip1_speed = max(hip1_speed, 0.5)\n",
      "        hip2_speed = max(hip2_speed, 0.5)\n",
      "\n",
      "    # Terrain Awareness\n",
      "    if lidar_1 > 0.5:\n",
      "        hip2_speed = -max(hip2_speed, 0.5)\n",
      "    if lidar_2 > 0.5:\n",
      "        hip1_speed = -max(hip1_speed, 0.5)\n",
      "\n",
      "    # Recovery Logic\n",
      "    if lidar_1 > 0.7:\n",
      "        hip1_speed = 0\n",
      "    if lidar_2 > 0.7:\n",
      "        hip2_speed = 0\n",
      "    if lidar_3 > 0.7:\n",
      "        hip3_speed = 0\n",
      "    if lidar_4 > 0.7:\n",
      "        hip4_speed = 0\n",
      "\n",
      "    return [hip1_speed, hip2_speed, hip3_speed, hip4_speed]\n",
      "--------------\n",
      "\n",
      "Function correctness reward:  [2.0]\n"
     ]
    }
   ],
   "source": [
    "input_to_reward = [[{\"content\": generated_function}]]\n",
    "print(f\"Strategy reward:  {strategy_succeeds(input_to_reward)}\")\n",
    "print(f\"Function correctness reward:  {function_works(input_to_reward)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7f8b2",
   "metadata": {},
   "source": [
    "\n",
    "## 11. GRPO Trainer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c33364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_prompt_length = maximum_length + 1\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "temperature=1.0,\n",
    "min_p=0.1,\n",
    "learning_rate=5e-6,\n",
    "weight_decay=0.01,\n",
    "warmup_ratio=0.1,\n",
    "lr_scheduler_type=\"linear\",\n",
    "optim=\"adamw_8bit\",\n",
    "logging_steps=1,\n",
    "per_device_train_batch_size=4,\n",
    "gradient_accumulation_steps=1,\n",
    "num_generations=4,\n",
    "max_prompt_length=max_prompt_length,\n",
    "max_completion_length=max_completion_length,\n",
    "max_steps=400,\n",
    "save_steps=50,\n",
    "report_to=\"none\",\n",
    "output_dir=\"outputs_llama_bipedal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4968e",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Initialize Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441a5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = GRPOTrainer(\n",
    "model=model,\n",
    "processing_class=tokenizer,\n",
    "reward_funcs=[\n",
    "strategy_succeeds,\n",
    "function_works,\n",
    "],\n",
    "args=training_args,\n",
    "train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196230d",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Save the Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf36492",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained_merged(\"llama_bipedal_final\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73c038-d6d2-4bd4-a39a-fb7ecf202f65",
   "metadata": {},
   "source": [
    "## 14. Evaluate finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6002bfa-8c64-48a3-90d0-5a5fd2e09156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    AMD Radeon Graphics. Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+rocm6.4. ROCm Toolkit: 6.4.43482-0f2d60242. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "model_finetuned, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./llama_bipedal_2\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deea2814-854e-4489-8ed0-c88985e2f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable faster inference\n",
    "model_finetuned = FastLanguageModel.for_inference(model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2ffb85-b6d5-4f60-9503-2729bab6d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">\n",
      "<|im_start|>assistant\n",
      "```python\n",
      "def strategy(obs) -> list[float]:\n",
      "    hull_angle = obs[0]\n",
      "    hull_angular_velocity = obs[1]\n",
      "    horizontal_speed = obs[2]\n",
      "    vertical_speed = abs(obs[3])  # take absolute value\n",
      "    hip1_angle, hip1_speed = obs[4:6]\n",
      "    hip2_angle, hip2_speed = obs[6:8]\n",
      "    leg1_contact, hip3_angle, hip3_speed = obs[8:11]\n",
      "    hip4_angle, hip4_speed = obs[11:13]\n",
      "    leg2_contact = obs[13]\n",
      "    lidar_1, lidar_2, lidar_3, lidar_4, lidar_5, lidar_6, lidar_7, lidar_8, lidar_9, lidar_10 = obs[14:25]\n",
      "\n",
      "    # Balance First\n",
      "    if abs(hull_angle) > 0.1:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "    else:\n",
      "        hip1_speed = 0.1\n",
      "        hip2_speed = -0.1\n",
      "\n",
      "    # Step Rhythmically\n",
      "    if leg1_contact:\n",
      "        hip3_speed = -0.1\n",
      "        hip4_speed = 0.1\n",
      "    else:\n",
      "        hip3_speed = 0\n",
      "        hip4_speed = 0\n",
      "\n",
      "    # Gentle Force\n",
      "    hip1_speed = max(-0.5, min(0.5, hip1_speed))\n",
      "    hip2_speed = max(-0.5, min(0.5, hip2_speed))\n",
      "    hip3_speed = max(-0.5, min(0.5, hip3_speed))\n",
      "    hip4_speed = max(-0.5, min(0.5, hip4_speed))\n",
      "\n",
      "    # Forward Motion\n",
      "    if horizontal_speed < 0:\n",
      "        hip1_speed = 0.1\n",
      "        hip2_speed = -0.1\n",
      "    else:\n",
      "        hip1_speed = -0.1\n",
      "        hip2_speed = 0.1\n",
      "\n",
      "    # Terrain Awareness\n",
      "    if lidar_1 > 0.5:\n",
      "        hip1_speed = 0.1\n",
      "        hip3_speed = -0.1\n",
      "    elif lidar_1 < -0.5:\n",
      "        hip1_speed = -0.1\n",
      "        hip3_speed = 0.1\n",
      "\n",
      "    # Recovery Logic\n",
      "    if hull_angle > 0.2:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "        hip3_speed = -0.1\n",
      "    elif hull_angle < -0.2:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "        hip3_speed = 0.1\n",
      "\n",
      "    return [hip1_speed, hip3_speed, hip2_speed, hip4_speed]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generated_function = generate_strategy(model_finetuned, tokenizer, [{\"role\": \"user\", \"content\": USER_PROMPT}],1024) \n",
    "print(generated_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d616e323-c5c6-4d23-8413-e1f00d42dfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran strategy for 83 steps -> Scored -115.99754700483554\n",
      "Strategy reward:  [8.125387918039765]\n",
      "def strategy(obs) -> list[float]:\n",
      "    hull_angle = obs[0]\n",
      "    hull_angular_velocity = obs[1]\n",
      "    horizontal_speed = obs[2]\n",
      "    vertical_speed = abs(obs[3])  # take absolute value\n",
      "    hip1_angle, hip1_speed = obs[4:6]\n",
      "    hip2_angle, hip2_speed = obs[6:8]\n",
      "    leg1_contact, hip3_angle, hip3_speed = obs[8:11]\n",
      "    hip4_angle, hip4_speed = obs[11:13]\n",
      "    leg2_contact = obs[13]\n",
      "    lidar_1, lidar_2, lidar_3, lidar_4, lidar_5, lidar_6, lidar_7, lidar_8, lidar_9, lidar_10 = obs[14:25]\n",
      "\n",
      "    # Balance First\n",
      "    if abs(hull_angle) > 0.1:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "    else:\n",
      "        hip1_speed = 0.1\n",
      "        hip2_speed = -0.1\n",
      "\n",
      "    # Step Rhythmically\n",
      "    if leg1_contact:\n",
      "        hip3_speed = -0.1\n",
      "        hip4_speed = 0.1\n",
      "    else:\n",
      "        hip3_speed = 0\n",
      "        hip4_speed = 0\n",
      "\n",
      "    # Gentle Force\n",
      "    hip1_speed = max(-0.5, min(0.5, hip1_speed))\n",
      "    hip2_speed = max(-0.5, min(0.5, hip2_speed))\n",
      "    hip3_speed = max(-0.5, min(0.5, hip3_speed))\n",
      "    hip4_speed = max(-0.5, min(0.5, hip4_speed))\n",
      "\n",
      "    # Forward Motion\n",
      "    if horizontal_speed < 0:\n",
      "        hip1_speed = 0.1\n",
      "        hip2_speed = -0.1\n",
      "    else:\n",
      "        hip1_speed = -0.1\n",
      "        hip2_speed = 0.1\n",
      "\n",
      "    # Terrain Awareness\n",
      "    if lidar_1 > 0.5:\n",
      "        hip1_speed = 0.1\n",
      "        hip3_speed = -0.1\n",
      "    elif lidar_1 < -0.5:\n",
      "        hip1_speed = -0.1\n",
      "        hip3_speed = 0.1\n",
      "\n",
      "    # Recovery Logic\n",
      "    if hull_angle > 0.2:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "        hip3_speed = -0.1\n",
      "    elif hull_angle < -0.2:\n",
      "        hip1_speed = 0\n",
      "        hip2_speed = 0\n",
      "        hip3_speed = 0.1\n",
      "\n",
      "    return [hip1_speed, hip3_speed, hip2_speed, hip4_speed]\n",
      "--------------\n",
      "\n",
      "Function correctness reward:  [2.0]\n"
     ]
    }
   ],
   "source": [
    "input_to_reward = [[{\"content\": generated_function}]]\n",
    "print(f\"Strategy reward:  {strategy_succeeds(input_to_reward)}\")\n",
    "print(f\"Function correctness reward:  {function_works(input_to_reward)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
